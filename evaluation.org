#+TITLE: Evaluation
#+AUTHOR: Vera Neplenbroek
#+DATE: Saturday, 03 April 2021
#+PROPERTY: header-args :exports both :session evaluate :cache no :results value

* Import
  #+begin_src python :results silent
import itertools
from phonemizer import phonemize
from tqdm import tqdm
  #+end_src

* Prepare data
First the |'s in the phonemized data need to be replaced with spaces,
so they can be processed correctly when the joint intent
classification and slot filling model is trained.

  #+begin_src python :results silent
for dataset in [
    "atis/train",
    "atis/test",
    "atis/valid",
    "snips/train",
    "snips/test",
    "snips/valid",
]:
    with open(f"data/{dataset}/seq_ref_in.phon", "r") as infile:
        text = [i.strip() for i in infile.readlines()]

    # replace | that are in between phonemes with spaces
    text = list(map(lambda st: str.replace(st, "|", " "), text))
    with open(f"data/{dataset}/seq_ref_in_processed", "w") as outfile:
        outfile.write("\n".join(text))

    # replace | that are in between phonemes with spaces
    with open(f"data/{dataset}/seq_asr_in.phon", "r") as infile:
        text = [i.strip() for i in infile.readlines()]

    text = list(map(lambda st: str.replace(st, "|", " "), text))
    with open(f"data/{dataset}/seq_asr_in_processed", "w") as outfile:
        outfile.write("\n".join(text))
  #+end_src

* Multiply labels
Now that I have trained phoneme embeddings, I get embedded utterances
that are as long as the number of phonemes in the utterance. The slot
labels are given per word however, which means I could either repeat
them as often as there are phonemes in a word (1) or find a different
way to deal with this in which multi input data is always supplied and
phoneme embeddings can work while the slots correspond to the given
word representations (2). I will first try approach one and evaluate
its performance. For this I need to create new multiplied slot labels
first:

#+begin_src python :results silent
special = {
    "vt": 2,
    "vl": 2,
    "i-lander": 2,
    "fm": 2,
    "pw": 2,
    "rx": 2,
    "ice-cream": 2,
    "eve-olution": 2,
    "awards/": 2,
    "camping-car": 2,
    "benson-meditation": 2,
    "bb": 2,
    "best-of:": 2,
    "sun-star": 2,
    "tx": 2,
    "tatra-nationalpark": 2,
    "saint-pierre": 2,
    "morgan-monroe": 2,
    "u-roy": 2,
    "suthep-pui": 2,
    "luambe-nationalpark": 2,
    "gf": 2,
    "b-sides": 2,
    "kd": 2,
    "pointe-heath": 2,
    "axis2": 2,
    "wv": 2,
    "live-legend": 2,
    "2gether": 2,
    "wide-eyed": 2,
    "nl": 2,
    "mf": 2,
    "cd-rom": 2,
    "broadway-lafayette": 2,
    "tallahassee-st": 2,
    "news-press": 2,
    "64:": 2,
    "21st": 2,
    "mh": 2,
    "half-life": 2,
    "holla-day": 2,
    "22nd": 2,
    "flesh-colored": 2,
    "ape-man": 2,
    "smith-9th": 2,
    "t-rex": 2,
    "nj": 2,
    "jd": 2,
    "cairngorms-nationalpark": 2,
    "nd": 2,
    "ks": 2,
    "twenty-seventh": 2,
    "pr": 2,
    "00s": 2,
    "e-type": 2,
    "hip-hop": 2,
    "dachigam-nationalpark": 2,
    "mp": 2,
    "mn": 2,
    "br": 2,
    "bt": 2,
    "post-grunge": 2,
    "band-e": 2,
    "k-pop": 2,
    "top-20": 2,
    "top-10": 2,
    "mid-sixties": 2,
    "59th": 2,
    "top-50": 2,
    "meeres-nationalpark": 2,
    "hundred-year": 2,
    "top-ten": 2,
    "park-beach": 2,
    "main-travelled": 2,
    "close-by": 2,
    "top-twenty": 2,
    "pop-folk": 2,
    "folk-rock": 2,
    "tea-time": 2,
    "twenty-sixth": 2,
    "two-shoes": 2,
    "hundred-foot": 2,
    "seven-ups": 2,
    "lexington-fayette": 2,
    "top-5": 2,
    "twenty-third": 2,
    "f-1": 2,
    "granite-steppe": 2,
    "top-rated": 2,
    "sq": 2,
    "'72": 2,
    "pre-party": 2,
    "ct": 2,
    "chrome-plated": 2,
    "a-hunting": 2,
    "kushiro-shitsugen": 2,
    "twenty-first": 2,
    "26th": 2,
    "peace-maker": 2,
    "twenty-fourth": 2,
    "twenty-fifth": 2,
    "black-body": 2,
    "dochu-kotsu": 2,
    "300:": 2,
    "bg": 2,
    "tn": 2,
    "7even": 2,
    "md": 2,
    "top-five": 2,
    "top-fifty": 2,
    "pt": 2,
    "kj": 2,
    "show-ya": 2,
    "nv": 2,
    "24th": 2,
    "cream-trilogie": 2,
    "tv": 2,
    "mj": 2,
    "deal:50": 2,
    "nm": 2,
    "fl": 2,
    "d-day": 2,
    "dj": 2,
    "guinea-bissau": 2,
    "his-story": 2,
    "cj": 2,
    "atanassow-see": 2,
    "pot-healer": 2,
    "twenty-second": 2,
    "bc": 2,
    "dc": 2,
    "qx": 2,
    "fr": 2,
    "97:": 2,
    "9am": 2,
    "drive-in": 2,
    "qo": 2,
    "fn": 2,
    "yn": 2,
    "qw": 2,
    "2010s": 2,
    "sb": 2,
    "van-pires": 2,
    "23rd": 2,
    "anti-japanese": 2,
    "show-biz": 2,
    "knife-throwing": 2,
    "v-the": 2,
    "joo-hyun": 2,
    "8-week": 2,
    "demi-gods": 2,
    "semi-devils": 2,
    "bn": 2,
    "cp": 2,
    "bh": 2,
    "bf": 2,
    "dl": 2,
    "sp": 2,
    "butt-head": 2,
    "sd": 2,
    "sc": 2,
    "twenty-eighth": 2,
    "hp": 2,
    "72s": 2,
    "72nd": 2,
    "25th": 2,
    "cash-cash": 2,
    "nh": 2,
    "italian-american": 2,
    "kb": 2,
    "dovre-nationalpark": 2,
    "nc": 2,
    "homem-christo": 2,
    "guy-manuel": 2,
    "bothe-napa": 2,
    "serving-men": 2,
    "camdeboo-nationalpark": 2,
    "fun-punk": 2,
    "88th": 2,
    "st-boyd": 2,
    "milton-freewater": 2,
    "fox-hunting": 2,
    "furano-ashibetsu": 2,
    "caledonian-record": 2,
    "el-palacio": 2,
    "fall-down": 2,
    "post-punk": 2,
    "volume4": 2,
    "av-barclays": 2,
    "73s": 2,
    "27th": 2,
    "cherry-tree": 2,
    "baby-sittor": 2,
    "rimsky-korsakoffee": 2,
    "mg": 2,
    "g-men": 2,
    "parthenais-perrault": 2,
    "wine-stained": 2,
    "rd": 2,
    "qr": 2,
    "black—victini": 2,
    "white—victini": 2,
    "space-age": 2,
    "tse-tung": 2,
    "4-hour": 2,
    "xl": 2,
    "bridge-city": 2,
    "she-devil": 2,
    "glacier-nationalpark": 2,
    "techno-industrial": 2,
    "be1": 2,
    "in-birth": 2,
    "a-z": 2,
    "jean-georges": 2,
    "harrison-crawford": 2,
    "opa-opa": 2,
    "ll": 2,
    "m-cabi": 2,
    "hyun-joong": 2,
    "28th": 2,
    "on-line": 2,
    "nimule-nationalpark": 2,
    "anti-semitism": 2,
    "co-operative": 2,
    "melcher-dallas": 2,
    "week-end": 2,
    "lo-fi": 2,
    "sci-fi": 2,
    "confidence-man": 2,
    "trip-hop": 2,
    "gipsy-gordon": 2,
    "shangri-la": 2,
    "north-west": 2,
    "ak-suu": 2,
    "fifty-five": 2,
    "seven-thirty": 2,
    "maid-rite": 2,
    "regulate…g": 2,
    "half-formed": 2,
    "rev-raptor": 2,
    "r&b": 3,
    "rmx": 3,
    "dhl": 3,
    "003½": 3,
    "cpn": 3,
    "a-myin-thit": 3,
    "av-69th": 3,
    "msf": 3,
    "pkk": 3,
    "207th": 3,
    "b&b": 3,
    "nmd": 3,
    "winnie-the-pooh": 3,
    "shw": 3,
    "cpv": 3,
    "msn": 3,
    "tkk": 3,
    "wpp": 3,
    "116th": 3,
    "cdc": 3,
    "cpc": 3,
    "nfl": 3,
    "bpm": 3,
    "l10": 3,
    "tvs": 3,
    "105th": 3,
    "100%": 3,
    "nsc": 3,
    "2k3": 3,
    "tdc": 3,
    "bhd": 3,
    "av-53rd": 3,
    "akb48": 3,
    "ndf": 3,
    "llp": 3,
    "jnr": 3,
    "ap80": 3,
    "mps": 3,
    "fmd": 3,
    "rbd": 3,
    "dfw": 3,
    "nld": 3,
    "whm": 3,
    "pdp": 3,
    "mls": 3,
    "ctr": 3,
    "vrt": 3,
    "cds": 3,
    "vnd": 3,
    "frl": 3,
    "prc": 3,
    "dvd": 3,
    "jfk": 3,
    "10:15": 3,
    "skg": 3,
    "ptt": 3,
    "gms": 3,
    "cpl": 3,
    "fpg": 3,
    "kbr": 3,
    "csf": 3,
    "cmc": 3,
    "tnk": 3,
    "bhp": 3,
    "kpn": 3,
    "kph": 3,
    "09:30": 3,
    "salaam-e-ishq:": 3,
    "pmd": 3,
    "06:13": 3,
    "00:17": 3,
    "technical&brutal": 3,
    "m80": 3,
    "mgm": 3,
    "nhl": 3,
    "mjr": 3,
    "14:40": 3,
    "d9s": 3,
    "trc": 3,
    "jtr": 3,
    "dpr": 3,
    "le-aqua-na": 3,
    "11:12": 3,
    "f28": 3,
    "06:30": 3,
    "kmt": 3,
    "lbc": 3,
    "cbk": 3,
    "cwb": 3,
    "mtv:": 3,
    "rss": 3,
    "j31": 3,
    "101:": 3,
    "dh8": 3,
    "dc9": 3,
    "spd": 3,
    "birthday/lincoln": 3,
    "altyn-emel-nationalpark": 3,
    "ntc": 3,
    "13:19": 3,
    "fdd": 3,
    "rnb": 3,
    "1634:": 3,
    "01:50": 3,
    "pow/mia": 3,
    "dtw": 3,
    "103rd": 3,
    "09:59": 4,
    "22:54": 4,
    "maliau-basin-conservation-area": 4,
    "06:59": 4,
    "15:04": 4,
    "07:25": 4,
    "21:49": 4,
    "spdc": 4,
    "18:28": 4,
    "dprk": 4,
    "sxsw": 4,
    "ndrc": 4,
    "02:31": 4,
    "03:43": 4,
    "11:56": 4,
    "10:37": 4,
    "09:42": 4,
    "00:00": 4,
    "00:55": 4,
    "12:26": 4,
    "04:45": 4,
    "14:41": 4,
    "01:27": 4,
    "dvds": 4,
    "cock-a-doodle-doo": 4,
    "08:39": 4,
    "n9ne": 4,
    "dc10": 4,
    "17:43": 4,
    "09:44": 4,
    "7/18/2030": 4,
    "19:26": 4,
    "20:38": 4,
    "00:32": 4,
    "06:42": 4,
    "blvd": 4,
    "08:05": 4,
    "07:07": 4,
    "20:44": 4,
    "19:52": 4,
    "gd&top": 4,
    "02:45": 4,
    "sncf": 4,
    "1/1/2018": 4,
    "1/11/2040": 4,
    "1/11/2030": 4,
    "135th": 4,
    "dlrs": 4,
    "138th": 4,
    "03:44": 4,
    "hsbc": 4,
    "11:09": 4,
    "02:22": 4,
    "15:02": 4,
    "847:": 4,
    "07:27": 4,
    "07:52": 4,
    "02:59": 4,
    "00:37": 4,
    "10:24": 4,
    "1-1000": 4,
    "12:06": 4,
    "2/6/2020": 4,
    "02:53": 4,
    "4/19/2030": 4,
    "12:53": 4,
    "06:18:13": 5,
    "4/3/2027": 5,
    "6/14/2035": 5,
    "7/10/2023": 5,
    "2/7/2021": 5,
    "1914-1918": 5,
    "4/15/2034": 5,
    "10:21:20": 5,
    "1/20/2023": 5,
    "03:19:13": 5,
    "10/4/2021": 5,
    "12/13/2025": 5,
    "12/10/2035": 5,
    "5/17/2037": 5,
    "8/8/2039": 5,
    "6/15/2025": 5,
    "4/20/2038": 5,
    "12/9/2039": 5,
    "4/17/2033": 5,
    "12/28/2019": 5,
    "02:02:30": 5,
    "10/22/2030": 5,
    "6/1/2027": 5,
    "cppcc": 5,
    "21:05:17": 5,
    "7/16/2032": 5,
    "5/20/2028": 5,
    "10:56:18": 5,
    "06:50:20": 5,
    "5/20/2025": 5,
    "1970-1980": 5,
    "12/26/2018": 5,
    "11/12/2036": 5,
    "17:32:30": 5,
    "11/1/2033": 5,
    "10/14/2026": 5,
    "05:44:13": 5,
    "8/4/2024": 5,
    "10/2/2021": 5,
    "7/22/2030": 5,
    "l1011": 5,
    "dcode2016": 5,
    "04:08:11": 5,
    "blvd-lehman": 5,
    "jkt48": 5,
    "04:34:15": 5,
    "3/21/2018": 5,
    "1/1/2031": 5,
    "18:49:20": 5,
    "12/14/2023": 5,
    "4/4/2036": 5,
    "7/16/2027": 5,
    "9/3/2034": 5,
    "15:26:11": 5,
    "7/13/2036": 5,
    "10:47:15": 5,
    "07:43:21": 6,
    "06:31:22": 6,
    "3/22/2038": 6,
    "01:19:00": 6,
    "15:16:52": 6,
    "08:56:29": 6,
    "2/25/2025": 6,
    "06:30:26": 6,
    "02:39:23": 6,
    "3/26/2023": 6,
    "11:47:52": 6,
    "6/21/2035": 6,
    "09:04:38": 6,
    "17:38:04": 6,
    "05:00:34": 6,
    "09:58:27": 6,
    "15:19:29": 6,
    "11:16:07": 6,
    "02:55:25": 6,
    "10/24/2028": 6,
    "10/21/2024": 6,
    "7/27/2036": 6,
    "00:09:07": 6,
    "03:01:48": 6,
    "22:23:22": 6,
    "11:36:48": 6,
    "04:36:28": 6,
    "13:22:34": 6,
    "10:41:51": 6,
    "1933-45": 6,
    "07:03:43": 6,
    "11/23/2031": 6,
    "20:45:24": 6,
    "19:44:58": 6,
    "07:08:02": 6,
    "em4jay": 6,
    "00:47:43": 6,
    "01:51:47": 6,
    "05:51:52": 6,
    "7/25/2027": 6,
    "07:31:32": 6,
    "2/21/2021": 6,
    "16:01:04": 6,
    "01:48:35": 6,
    "8/26/2022": 6,
    "07:08:00": 6,
    "21:41:08": 6,
    "09:32:06": 6,
    "2003-2013": 7,
    "1997-2003": 8,
    "1994-2009": 8,
    "2007-2008": 9,
    "crazy=genius": 12,
}
#+end_src

#+begin_src python
for dataset in [
    "atis/train",
    "atis/test",
    "atis/valid",
    "snips/train",
    "snips/test",
    "snips/valid",
]:
    with open(f"data/{dataset}/seq.out", "r") as infile:
        slot_labels = [i.strip().split() for i in infile.readlines()]

    with open(f"data/{dataset}/seq_ref_in.phon", "r") as infile:
        ref_phonemes = [i.strip().split() for i in infile.readlines()]

    with open(f"data/{dataset}/seq_ref.in", "r") as infile:
        ref_sequences = [i.strip().split() for i in infile.readlines()]

    with open(f"data/{dataset}/seq_asr_in.phon", "r") as infile:
        asr_phonemes = [i.strip().split() for i in infile.readlines()]

    with open(f"data/{dataset}/seq_asr.in", "r") as infile:
        asr_sequences = [i.strip().split() for i in infile.readlines()]

    # list of lists of the number of phonemes each word in a ref sequence
    # contains
    rep_ref = []
    for i in tqdm(range(len(ref_phonemes))):
        # list of the number of phonemes each word in a sequence contains
        add = []
        for j in range(len(ref_phonemes[i])):
            add.append(len(ref_phonemes[i][j].split("|")) - 1)

        # if a phonemized word consists of multiple words, correct for it
        for j in range(len(ref_sequences[i])):
            if ref_sequences[i][j] in special.keys():
                add = (
                    add[:j]
                    + [sum(add[j : j + special[ref_sequences[i][j]]])]
                    + add[j + special[ref_sequences[i][j]] :]
                )

            elif (
                ref_sequences[i][j] == "1765"
                and ref_sequences[i][j - 1] == "continental"
            ):
                add = add[:j] + [sum(add[j : j + 6])] + add[j + 6 :]

            elif ref_sequences[i][j] == "19" and ref_sequences[i][j - 1] == "flight":
                add = add[:j] + [sum(add[j : j + 2])] + add[j + 2 :]

            elif ref_sequences[i][j] == "2016" and ref_sequences[i][j - 1] == "48":
                add = add[:j] + [sum(add[j : j + 4])] + add[j + 4 :]

            elif ref_sequences[i][j] == "2034" and ref_sequences[i][j - 1] == "8":
                add = add[:j] + [sum(add[j : j + 4])] + add[j + 4 :]

            elif ref_sequences[i][j] == "1500" and ref_sequences[i][j - 1] == "us":
                add = add[:j] + [sum(add[j : j + 4])] + add[j + 4 :]

            elif (
                ref_sequences[i][j] == "2036"
                and ref_sequences[i][j - 1] == "24"
                and ref_sequences[i][j - 2] == "nova"
            ):
                add = add[:j] + [sum(add[j : j + 4])] + add[j + 4 :]

            elif (
                ref_sequences[i][j] == "2024"
                and ref_sequences[i][j - 1] == "24"
                and ref_sequences[i][j - 2] == "joon"
            ):
                add = add[:j] + [sum(add[j : j + 4])] + add[j + 4 :]

            elif ref_sequences[i][j] == "2020" and ref_sequences[i][j - 1] == "8":
                add = add[:j] + [sum(add[j : j + 4])] + add[j + 4 :]

            elif ref_sequences[i][j] == "2019" and ref_sequences[i][j - 1] == "8":
                add = add[:j] + [sum(add[j : j + 4])] + add[j + 4 :]

            elif ref_sequences[i][j] == "2034" and ref_sequences[i][j - 1] == "6":
                add = add[:j] + [sum(add[j : j + 4])] + add[j + 4 :]

            elif ref_sequences[i][j] == "2023" and ref_sequences[i][j - 1] == "7":
                add = add[:j] + [sum(add[j : j + 4])] + add[j + 4 :]

            elif ref_sequences[i][j] == "2029" and ref_sequences[i][j - 1] == "18":
                add = add[:j] + [sum(add[j : j + 4])] + add[j + 4 :]

            if ref_sequences[i][j] == "18" and j != (len(ref_sequences[i]) - 1):
                if ref_sequences[i][j + 1] == "2029":
                    add = add[:j] + [sum(add[j : j + 2])] + add[j + 2 :]

            elif ref_sequences[i][j] == "2038" and ref_sequences[i][j - 1] == "16":
                add = add[:j] + [sum(add[j : j + 4])] + add[j + 4 :]

            if ref_sequences[i][j] == "16" and j != (len(ref_sequences[i]) - 1):
                if ref_sequences[i][j + 1] == "2038":
                    add = add[:j] + [sum(add[j : j + 2])] + add[j + 2 :]

            elif ref_sequences[i][j] == "2024" and ref_sequences[i][j - 1] == "3":
                add = add[:j] + [sum(add[j : j + 4])] + add[j + 4 :]

            elif ref_sequences[i][j] == "2035" and ref_sequences[i][j - 1] == "24":
                add = add[:j] + [sum(add[j : j + 4])] + add[j + 4 :]

            elif ref_sequences[i][j] == "2027" and ref_sequences[i][j - 1] == "26":
                add = add[:j] + [sum(add[j : j + 4])] + add[j + 4 :]

            elif ref_sequences[i][j] == "2035" and ref_sequences[i][j - 1] == "21":
                add = add[:j] + [sum(add[j : j + 4])] + add[j + 4 :]

            if ref_sequences[i][j] == "16" and j != (len(ref_sequences[i]) - 1):
                if ref_sequences[i][j + 1] == "2036":
                    add = add[:j] + [sum(add[j : j + 2])] + add[j + 2 :]

            elif ref_sequences[i][j] == "2036" and ref_sequences[i][j - 1] == "16":
                add = add[:j] + [sum(add[j : j + 4])] + add[j + 4 :]

            elif (
                ref_sequences[i][j] == "1970"
                and ref_sequences[i][j - 1] == "classics"
                and ref_sequences[i][j + 1] == "to"
                and ref_sequences[i][j + 2] == "1975"
            ):
                add = add[:j] + [sum(add[j : j + 5])] + add[j + 5 :]

            elif (
                ref_sequences[i][j] == "1970"
                and ref_sequences[i][j - 1] == "glass"
                and ref_sequences[i][j + 1] == "to"
                and ref_sequences[i][j + 2] == "1975"
            ):
                add = add[:j] + [sum(add[j : j + 5])] + add[j + 5 :]

            elif (
                ref_sequences[i][j] == "1970"
                and ref_sequences[i][j - 1] == "catholics"
                and ref_sequences[i][j + 1] == "to"
                and ref_sequences[i][j + 2] == "1975"
            ):
                add = add[:j] + [sum(add[j : j + 5])] + add[j + 5 :]

            elif (
                ref_sequences[i][j] == "1967"
                and ref_sequences[i][j - 1] == "mixtape"
                and ref_sequences[i][j + 1] == "to"
                and ref_sequences[i][j + 2] == "1975"
            ):
                add = add[:j] + [sum(add[j : j + 6])] + add[j + 6 :]

            elif ref_sequences[i][j].isdigit():
                add = (
                    add[:j]
                    + [sum(add[j : j + len(phonemize(ref_sequences[i][j]).split())])]
                    + add[j + len(phonemize(ref_sequences[i][j]).split()) :]
                )

        rep_ref.append(add)

    # repeat each label as many times as the corresponding word has phonemes
    slot_labels_ref = []
    for i in range(len(rep_ref)):
        slot_labels_ref.append(
            " ".join(
                list(
                    itertools.chain(
                        ,*(
                            itertools.repeat(elem, n)
                            for elem, n in zip(slot_labels[i], rep_ref[i])
                        )
                    )
                )
            )
        )

    # list of lists of the number of phonemes each word in an asr sequence
    # contains
    rep_asr = []
    for i in tqdm(range(len(asr_phonemes))):
        # list of the number of phonemes each word in a sequence contains
        add = []
        for j in range(len(asr_phonemes[i])):
            add.append(len(asr_phonemes[i][j].split("|")) - 1)

        for j in range(len(asr_sequences[i])):
        # if a phonemized word consists of multiple words, correct for it
            if asr_sequences[i][j] in special.keys():
                add = (
                    add[:j]
                    + [sum(add[j : j + special[asr_sequences[i][j]]])]
                    + add[j + special[asr_sequences[i][j]] :]
                )

            elif (
                asr_sequences[i][j] == "1765"
                and asr_sequences[i][j - 1] == "continental"
            ):
                add = add[:j] + [sum(add[j : j + 6])] + add[j + 6 :]

            elif asr_sequences[i][j] == "19" and asr_sequences[i][j - 1] == "flight":
                add = add[:j] + [sum(add[j : j + 2])] + add[j + 2 :]

            elif asr_sequences[i][j] == "2016" and asr_sequences[i][j - 1] == "48":
                add = add[:j] + [sum(add[j : j + 4])] + add[j + 4 :]

            elif asr_sequences[i][j] == "2034" and asr_sequences[i][j - 1] == "8":
                add = add[:j] + [sum(add[j : j + 4])] + add[j + 4 :]

            elif asr_sequences[i][j] == "1500" and asr_sequences[i][j - 1] == "us":
                add = add[:j] + [sum(add[j : j + 4])] + add[j + 4 :]

            elif (
                asr_sequences[i][j] == "2036"
                and asr_sequences[i][j - 1] == "24"
                and asr_sequences[i][j - 2] == "nova"
            ):
                add = add[:j] + [sum(add[j : j + 4])] + add[j + 4 :]

            elif (
                asr_sequences[i][j] == "2024"
                and asr_sequences[i][j - 1] == "24"
                and asr_sequences[i][j - 2] == "joon"
            ):
                add = add[:j] + [sum(add[j : j + 4])] + add[j + 4 :]

            elif asr_sequences[i][j] == "2020" and asr_sequences[i][j - 1] == "8":
                add = add[:j] + [sum(add[j : j + 4])] + add[j + 4 :]

            elif asr_sequences[i][j] == "2029" and asr_sequences[i][j - 1] == "18":
                add = add[:j] + [sum(add[j : j + 4])] + add[j + 4 :]

            elif asr_sequences[i][j] == "2019" and asr_sequences[i][j - 1] == "8":
                add = add[:j] + [sum(add[j : j + 4])] + add[j + 4 :]

            if asr_sequences[i][j] == "18" and j != (len(asr_sequences[i]) - 1):
                if asr_sequences[i][j + 1] == "2029":
                    add = add[:j] + [sum(add[j : j + 2])] + add[j + 2 :]

            if asr_sequences[i][j] == "16" and j != (len(asr_sequences[i]) - 1):
                if asr_sequences[i][j + 1] == "2036":
                    add = add[:j] + [sum(add[j : j + 2])] + add[j + 2 :]

            elif asr_sequences[i][j] == "2036" and asr_sequences[i][j - 1] == "16":
                add = add[:j] + [sum(add[j : j + 4])] + add[j + 4 :]

            elif asr_sequences[i][j] == "2038" and asr_sequences[i][j - 1] == "16":
                add = add[:j] + [sum(add[j : j + 4])] + add[j + 4 :]

            elif asr_sequences[i][j] == "2035" and asr_sequences[i][j - 1] == "24":
                add = add[:j] + [sum(add[j : j + 4])] + add[j + 4 :]

            if asr_sequences[i][j] == "16" and j != (len(asr_sequences[i]) - 1):
                if asr_sequences[i][j + 1] == "2038":
                    add = add[:j] + [sum(add[j : j + 2])] + add[j + 2 :]

            elif asr_sequences[i][j] == "2034" and asr_sequences[i][j - 1] == "6":
                add = add[:j] + [sum(add[j : j + 4])] + add[j + 4 :]

            elif asr_sequences[i][j] == "2035" and asr_sequences[i][j - 1] == "21":
                add = add[:j] + [sum(add[j : j + 4])] + add[j + 4 :]

            elif asr_sequences[i][j] == "2023" and asr_sequences[i][j - 1] == "7":
                add = add[:j] + [sum(add[j : j + 4])] + add[j + 4 :]

            elif asr_sequences[i][j] == "2024" and asr_sequences[i][j - 1] == "3":
                add = add[:j] + [sum(add[j : j + 4])] + add[j + 4 :]

            elif asr_sequences[i][j] == "2027" and asr_sequences[i][j - 1] == "26":
                add = add[:j] + [sum(add[j : j + 4])] + add[j + 4 :]

            elif (
                asr_sequences[i][j] == "1970"
                and asr_sequences[i][j - 1] == "classics"
                and asr_sequences[i][j + 1] == "to"
                and asr_sequences[i][j + 2] == "1975"
            ):
                add = add[:j] + [sum(add[j : j + 5])] + add[j + 5 :]

            elif (
                asr_sequences[i][j] == "1970"
                and asr_sequences[i][j - 1] == "glass"
                and asr_sequences[i][j + 1] == "to"
                and asr_sequences[i][j + 2] == "1975"
            ):
                add = add[:j] + [sum(add[j : j + 5])] + add[j + 5 :]

            elif (
                asr_sequences[i][j] == "1970"
                and asr_sequences[i][j - 1] == "catholics"
                and asr_sequences[i][j + 1] == "to"
                and asr_sequences[i][j + 2] == "1975"
            ):
                add = add[:j] + [sum(add[j : j + 5])] + add[j + 5 :]

            elif (
                asr_sequences[i][j] == "1967"
                and asr_sequences[i][j - 1] == "mixtape"
                and asr_sequences[i][j + 1] == "to"
                and asr_sequences[i][j + 2] == "1975"
            ):
                add = add[:j] + [sum(add[j : j + 6])] + add[j + 6 :]

            elif asr_sequences[i][j].isdigit():
                add = (
                    add[:j]
                    + [sum(add[j : j + len(phonemize(asr_sequences[i][j]).split())])]
                    + add[j + len(phonemize(asr_sequences[i][j]).split()) :]
                )

        rep_asr.append(add)

    # repeat each label as many times as the corresponding word has phonemes
    slot_labels_asr = []
    for i in range(len(rep_asr)):
        slot_labels_asr.append(
            " ".join(
                list(
                    itertools.chain(
                        ,*(
                            itertools.repeat(elem, n)
                            for elem, n in zip(slot_labels[i], rep_asr[i])
                        )
                    )
                )
            )
        )

    with open(f"data/{dataset}/seq_ref.out", "w") as outfile:
        outfile.write("\n".join(slot_labels_ref))

    with open(f"data/{dataset}/seq_asr.out", "w") as outfile:
        outfile.write("\n".join(slot_labels_asr))


(
    slot_labels[0],
    ref_phonemes[0],
    rep_ref[0],
    slot_labels_ref[0],
)
#+end_src

Now we can print all sentences that are not correctly processed yet:

#+begin_src python
for dataset in [
    "atis/train",
    "atis/test",
    "atis/valid",
    "snips/train",
    "snips/test",
    "snips/valid",
]:
    with open(f"data/{dataset}/seq_ref.out", "r") as infile:
        ref_slot_labels = [i.strip().split() for i in infile.readlines()]

    with open(f"data/{dataset}/seq_ref_in_processed", "r") as infile:
        ref_phonemes = [i.strip().split() for i in infile.readlines()]

    with open(f"data/{dataset}/seq_asr.out", "r") as infile:
        asr_slot_labels = [i.strip().split() for i in infile.readlines()]

    with open(f"data/{dataset}/seq_asr_in_processed", "r") as infile:
        asr_phonemes = [i.strip().split() for i in infile.readlines()]

    for i in tqdm(range(len(ref_slot_labels))):
        if len(ref_slot_labels[i]) != len(ref_phonemes[i]):
            print(ref_phonemes[i])

    for i in tqdm(range(len(asr_slot_labels))):
        if len(asr_slot_labels[i]) != len(asr_phonemes[i]):
            print(asr_phonemes[i])
#+end_src
