#+TITLE: Train Seq2Seq phoneme embeddings
#+AUTHOR: Vera Neplenbroek
#+DATE: Monday, 15 March 2021
#+PROPERTY: header-args :exports both :session phoneme_emb :cache no :results value

* Imports
  #+begin_src python :results silent
from io import open
import string
import re
import random

import time
import math

import pickle
import pandas as pd
from sklearn.manifold import TSNE
import numpy as np
import torch
import torch.nn as nn
from torch import optim
import torch.nn.functional as F

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
  #+end_src

* Data
First I load in the training data in their phonemized versions:

  #+begin_src python :results silent
with open(
    "data/phoneme_embedding_training_set/squad_samples_reference.phon", "r"
) as infile:
    squad_ref_phon = [i.strip() for i in infile.readlines()]
with open("data/phoneme_embedding_training_set/squad_samples_asr.phon", "r") as infile:
    squad_asr_phon = [i.strip() for i in infile.readlines()]
with open(
    "data/phoneme_embedding_training_set/subj_samples_reference.phon", "r"
) as infile:
    subj_ref_phon = [i.strip() for i in infile.readlines()]
with open("data/phoneme_embedding_training_set/subj_samples_asr.phon", "r") as infile:
    subj_asr_phon = [i.strip() for i in infile.readlines()]
  #+end_src

Now let's have a look at the reference and asr versions of the two
datasets:

  #+begin_src python
squad_ref_phon[0]
  #+end_src

  #+RESULTS:
  : hh|aw|s| ah|v| d|eh|r|ey|ax|n| b|ih|k|ey|m| n|ow|n| th|r|uw| b|ey|ax|n|s| ae|n|d| w|ih|ch| ah|v| b|ey|ax|n|s| ax|s| r|eh|l|ax|t|ih|v|z|

  #+begin_src python
squad_asr_phon[0]
  #+end_src

  #+RESULTS:
  : hh|aw|s| ah|v| d|eh|r|ey|ax|n| b|ih|k|ey|m| n|ow|n| th|r|uw| b|ey|ax|n|s| ae|n|d| w|ih|ch| ah|v| b|ih|aa|n|d| s|ey| eh|s| r|eh|l|ax|t|ih|v|z|

  #+begin_src python
subj_ref_phon[0]
  #+end_src

  #+RESULTS:
  : dh|ax| m|uw|v|iy| b|ax|g|ih|n|z| ih|n| dh|ax| p|ae|s|t| w|eh|r| ax| y|ah|ng| b|oy| n|ey|m|d| s|ae|m| ax|t|eh|m|p|t|s| t|ax| s|ey|v| s|eh|l|ey|b|iy| f|r|ah|m| ax| hh|ah|n|t|er|

  #+begin_src python
subj_asr_phon[0]
  #+end_src

  #+RESULTS:
  : dh|ax| m|uw|v|iy| b|ax|g|ih|n|z| ih|n| dh|ax| p|ae|s|t| w|eh|r| ax| y|ah|ng| b|oy| n|ey|m|d| s|ae|m| ax|t|eh|m|p|t|s| t|ax| s|ey|v| s|eh|l|ax|b| iy| f|r|ah|m| dh|ax| hh|ah|n|t|er|

Now I will add the two datasets together, since I will use a mix of
both for training. It is important that the order of the lists within
the lists, corresponding to sentences, stays the same.

  #+begin_src python
train_ref = squad_ref_phon + subj_ref_phon
train_asr = squad_asr_phon + subj_asr_phon
(len(train_ref), len(train_asr))
  #+end_src

  #+RESULTS:
  | 14715 | 14715 |

I put the padding symbol 'pad' in between two words:

  #+begin_src python
train_ref = list(map(lambda st: str.replace(st, " ", " pad "), train_ref))
train_asr = list(map(lambda st: str.replace(st, " ", " pad "), train_asr))
train_ref[0]
  #+end_src

  #+RESULTS:
  : hh|aw|s| pad ah|v| pad d|eh|r|ey|ax|n| pad b|ih|k|ey|m| pad n|ow|n| pad th|r|uw| pad b|ey|ax|n|s| pad ae|n|d| pad w|ih|ch| pad ah|v| pad b|ey|ax|n|s| pad ax|s| pad r|eh|l|ax|t|ih|v|z|

Since I will split the data into individual phonemes anyway and I do
not pay any attention to the word boundaries, I replace the phoneme
seperators by spaces:

  #+begin_src python
train_ref = list(map(lambda st: str.replace(st, "|", " "), train_ref))
train_asr = list(map(lambda st: str.replace(st, "|", " "), train_asr))
train_ref[0]
  #+end_src

  #+RESULTS:
  : hh aw s  pad ah v  pad d eh r ey ax n  pad b ih k ey m  pad n ow n  pad th r uw  pad b ey ax n s  pad ae n d  pad w ih ch  pad ah v  pad b ey ax n s  pad ax s  pad r eh l ax t ih v z

This means I can now split the sentences into sequences of phonemes
and padding symbols:

  #+begin_src python
train_ref = [sentence.split() for sentence in train_ref]
train_asr = [sentence.split() for sentence in train_asr]
train_ref[0]
  #+end_src

  #+RESULTS:
  | hh | aw | s | pad | ah | v | pad | d | eh | r | ey | ax | n | pad | b | ih | k | ey | m | pad | n | ow | n | pad | th | r | uw | pad | b | ey | ax | n | s | pad | ae | n | d | pad | w | ih | ch | pad | ah | v | pad | b | ey | ax | n | s | pad | ax | s | pad | r | eh | l | ax | t | ih | v | z |

* s2s

Code from https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html.

  #+begin_src python :results silent
SOS_token = 0
EOS_token = 1
MAX_LENGTH = max(
    max(map(lambda x: len(x), train_ref)), max(map(lambda x: len(x), train_asr))
) + 1


class Lang:
    def __init__(self, name):
        self.name = name
        self.phoneme2index = {}
        self.phoneme2count = {}
        self.index2phoneme = {0: "SOS", 1: "EOS"}
        self.n_phonemes = 2

    def addPhonemeList(self, phoneme_list):
        for phoneme in phoneme_list:
            if phoneme not in self.phoneme2index:
                self.phoneme2index[phoneme] = self.n_phonemes
                self.phoneme2count[phoneme] = 1
                self.index2phoneme[self.n_phonemes] = phoneme
                self.n_phonemes += 1

            else:
                self.phoneme2count[phoneme] += 1
  #+end_src

  #+begin_src python :results silent
def readLangs(lang1, lang2):
    pairs = [[train_ref[i], train_asr[i]] for i in range(len(train_ref))]
    input_lang = Lang(lang1)
    output_lang = Lang(lang2)
    return input_lang, output_lang, pairs
  #+end_src

  #+begin_src python
input_lang, output_lang, pairs = readLangs("ref", "asr")
for pair in pairs:
    input_lang.addPhonemeList(pair[0])
    output_lang.addPhonemeList(pair[1])

(input_lang.name, input_lang.n_phonemes, output_lang.name, output_lang.n_phonemes)
  #+end_src

  #+RESULTS:
  | ref | 43 | asr | 43 |

  #+begin_src python :results silent
class EncoderRNN(nn.Module):
    def __init__(self, input_size, hidden_size):
        super(EncoderRNN, self).__init__()
        self.hidden_size = hidden_size
        self.embedding = nn.Embedding(input_size, hidden_size)
        self.gru = nn.GRU(hidden_size, hidden_size)

    def forward(self, input, hidden):
        embedded = self.embedding(input).view(1, 1, -1)
        output = embedded
        output, hidden = self.gru(output, hidden)
        return output, hidden

    def initHidden(self):
        return torch.zeros(1, 1, self.hidden_size, device=device)
  #+end_src

  #+begin_src python :results silent
class DecoderRNN(nn.Module):
    def __init__(self, hidden_size, output_size):
        super(DecoderRNN, self).__init__()
        self.hidden_size = hidden_size
        self.embedding = nn.Embedding(output_size, hidden_size)
        self.gru = nn.GRU(hidden_size, hidden_size)
        self.out = nn.Linear(hidden_size, output_size)
        self.softmax = nn.Softmax(dim=1)

    def forward(self, input, hidden):
        output = self.embedding(input).view(1, 1, -1)
        output = F.relu(output)
        output, hidden = self.gru(output, hidden)
        output = self.softmax(self.out(output[0]))
        return output, hidden

    def initHidden(self):
        return torch.zeros(1, 1, self.hidden_size, device=device)
  #+end_src

  #+begin_src python :results silent
def indexesFromSentence(lang, sentence):
    return [lang.phoneme2index[phoneme] for phoneme in sentence]


def tensorFromSentence(lang, sentence):
    indexes = indexesFromSentence(lang, sentence)
    indexes.append(EOS_token)
    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)


def tensorsFromPair(pair):
    input_tensor = tensorFromSentence(input_lang, pair[0])
    target_tensor = tensorFromSentence(output_lang, pair[1])
    return (input_tensor, target_tensor)
  #+end_src

  #+begin_src python :results silent
def train(
    input_tensor,
    target_tensor,
    encoder,
    decoder,
    encoder_optimizer,
    decoder_optimizer,
    criterion,
    max_length=MAX_LENGTH,
):
    encoder_hidden = encoder.initHidden()
    encoder_optimizer.zero_grad()
    decoder_optimizer.zero_grad()
    input_length = input_tensor.size(0)
    target_length = target_tensor.size(0)
    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)
    loss = 0
    for ei in range(input_length):
        encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)
        encoder_outputs[ei] = encoder_output[0, 0]

    decoder_input = torch.tensor([[SOS_token]], device=device)
    decoder_hidden = encoder_hidden
    for di in range(target_length):
        decoder_output, decoder_hidden = decoder(
            decoder_input, decoder_hidden
        )
        topv, topi = decoder_output.topk(1)
        decoder_input = topi.squeeze().detach()  # detach from history as input
        loss += criterion(decoder_output, target_tensor[di])
        if decoder_input.item() == EOS_token:
            break

    loss.backward()
    encoder_optimizer.step()
    decoder_optimizer.step()
    return loss.item() / target_length
  #+end_src

  #+begin_src python :results silent
def asMinutes(s):
    m = math.floor(s / 60)
    s -= m * 60
    return '%dm %ds' % (m, s)


def timeSince(since, percent):
    now = time.time()
    s = now - since
    es = s / (percent)
    rs = es - s
    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))
  #+end_src

  #+begin_src python :results silent
def trainIters(
    encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01
):
    start = time.time()
    plot_losses = []
    print_loss_total = 0  # Reset every print_every
    plot_loss_total = 0  # Reset every plot_every
    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)
    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)
    training_pairs = [tensorsFromPair(random.choice(pairs)) for i in range(n_iters)]
    criterion = nn.CrossEntropyLoss()
    for iter in range(1, n_iters + 1):
        training_pair = training_pairs[iter - 1]
        input_tensor = training_pair[0]
        target_tensor = training_pair[1]
        loss = train(
            input_tensor,
            target_tensor,
            encoder,
            decoder,
            encoder_optimizer,
            decoder_optimizer,
            criterion,
        )
        print_loss_total += loss
        plot_loss_total += loss

        if iter % print_every == 0:
            print_loss_avg = print_loss_total / print_every
            print_loss_total = 0
            print(
                "%s (%d %d%%) %.4f"
                % (
                    timeSince(start, iter / n_iters),
                    iter,
                    iter / n_iters * 100,
                    print_loss_avg,
                )
            )

        if iter % plot_every == 0:
            plot_loss_avg = plot_loss_total / plot_every
            plot_losses.append(plot_loss_avg)
            plot_loss_total = 0
  #+end_src

  #+begin_src python :results silent
hidden_size = 20
encoder1 = EncoderRNN(input_lang.n_phonemes, hidden_size).to(device)
decoder1 = DecoderRNN(hidden_size, output_lang.n_phonemes).to(device)

trainIters(encoder1, decoder1, 75000, print_every=5000)
  #+end_src

  #+begin_src python :results silent
with open("encoder.pkl", "wb") as output_file:
     pickle.dump(encoder1, output_file)

with open("decoder.pkl", "wb") as output_file:
     pickle.dump(decoder1, output_file)
  #+end_src

* Visualization

   #+begin_src python
phonemes = list(input_lang.phoneme2index.keys())
X = decoder1.embedding(torch.LongTensor(np.arange(2, 43)))
#To create a numpy array out of the tensor:
X = X.detach().numpy()

(phonemes, X.shape)
   #+end_src

   #+RESULTS:
   | hh | aw | s | pad | ah | v | d | eh | r | ey | ax | n | b | ih | k | m | ow | th | uw | ae | w | ch | l | t | z | aa | f | ao | er | p | sh | ng | ay | uh | y | iy | g | dh | jh | oy | zh |
   | 41 | 20 |   |     |    |   |   |    |   |    |    |   |   |    |   |   |    |    |    |    |   |    |   |   |   |    |   |    |    |   |    |    |    |    |   |    |   |    |    |    |    |

   #+begin_src python
tsne = TSNE(n_components=2)
X_tsne = tsne.fit_transform(X)
df = pd.DataFrame(X_tsne, index=phonemes, columns=["x", "y"])

df.shape
   #+end_src

   #+RESULTS:
   | 41 | 2 |

   #+begin_src python :results silent
def annotate(row, ax):
    ax.annotate(row.name, (row.x, row.y),
                xytext=(10, -5), textcoords='offset points')
   #+end_src

   #+begin_src python :results silent
ax1 = df.plot.scatter(x="x", y="y")
df.apply(annotate, ax=ax1, axis=1)
fig = ax1.get_figure()
fig.savefig("s2s.png")
   #+end_src
