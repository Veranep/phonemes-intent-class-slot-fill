#+TITLE: Train Seq2Seq phoneme embeddings
#+AUTHOR: Vera Neplenbroek
#+DATE: Monday, 15 March 2021
#+PROPERTY: header-args :exports both :session phoneme_emb :cache no :results value

* Imports
  #+begin_src python :results silent
import copy
from io import open
import string
import re
import random

import time
import math

import pickle
import pandas as pd
from sklearn.manifold import TSNE
import numpy as np
import torch
import torch.nn as nn
from torch import optim
import torch.nn.functional as F

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
  #+end_src

* Data
** SQuAD and Subj
First I load in the training data in their phonemized versions:

  #+begin_src python :results silent
with open(
    "data/phoneme_embedding_training_set/squad_samples_reference.phon", "r"
) as infile:
    squad_ref_phon = [i.strip() for i in infile.readlines()]
with open("data/phoneme_embedding_training_set/squad_samples_asr.phon", "r") as infile:
    squad_asr_phon = [i.strip() for i in infile.readlines()]
with open(
    "data/phoneme_embedding_training_set/subj_samples_reference.phon", "r"
) as infile:
    subj_ref_phon = [i.strip() for i in infile.readlines()]
with open("data/phoneme_embedding_training_set/subj_samples_asr.phon", "r") as infile:
    subj_asr_phon = [i.strip() for i in infile.readlines()]
  #+end_src

Now let's have a look at the reference and asr versions of the two
datasets:

  #+begin_src python
squad_ref_phon[0]
  #+end_src

  #+RESULTS:
  : hh|aw|s| ah|v| d|eh|r|ey|ax|n| b|ih|k|ey|m| n|ow|n| th|r|uw| b|ey|ax|n|s| ae|n|d| w|ih|ch| ah|v| b|ey|ax|n|s| ax|s| r|eh|l|ax|t|ih|v|z|

  #+begin_src python
squad_asr_phon[0]
  #+end_src

  #+RESULTS:
  : hh|aw|s| ah|v| d|eh|r|ey|ax|n| b|ih|k|ey|m| n|ow|n| th|r|uw| b|ey|ax|n|s| ae|n|d| w|ih|ch| ah|v| b|ih|aa|n|d| s|ey| eh|s| r|eh|l|ax|t|ih|v|z|

  #+begin_src python
subj_ref_phon[0]
  #+end_src

  #+RESULTS:
  : dh|ax| m|uw|v|iy| b|ax|g|ih|n|z| ih|n| dh|ax| p|ae|s|t| w|eh|r| ax| y|ah|ng| b|oy| n|ey|m|d| s|ae|m| ax|t|eh|m|p|t|s| t|ax| s|ey|v| s|eh|l|ey|b|iy| f|r|ah|m| ax| hh|ah|n|t|er|

  #+begin_src python
subj_asr_phon[0]
  #+end_src

  #+RESULTS:
  : dh|ax| m|uw|v|iy| b|ax|g|ih|n|z| ih|n| dh|ax| p|ae|s|t| w|eh|r| ax| y|ah|ng| b|oy| n|ey|m|d| s|ae|m| ax|t|eh|m|p|t|s| t|ax| s|ey|v| s|eh|l|ax|b| iy| f|r|ah|m| dh|ax| hh|ah|n|t|er|

Now I will add the two datasets together, since I will use a mix of
both for training. It is important that the order of the lists within
the lists, corresponding to sentences, stays the same.

  #+begin_src python
train_ref = squad_ref_phon + subj_ref_phon
train_asr = squad_asr_phon + subj_asr_phon
(len(train_ref), len(train_asr))
  #+end_src

  #+RESULTS:
  | 14715 | 14715 |

I put the padding symbol 'pad' in between two words:

  #+begin_src python
train_ref = list(map(lambda st: str.replace(st, " ", " pad "), train_ref))
train_asr = list(map(lambda st: str.replace(st, " ", " pad "), train_asr))
train_ref[0]
  #+end_src

  #+RESULTS:
  : hh|aw|s| pad ah|v| pad d|eh|r|ey|ax|n| pad b|ih|k|ey|m| pad n|ow|n| pad th|r|uw| pad b|ey|ax|n|s| pad ae|n|d| pad w|ih|ch| pad ah|v| pad b|ey|ax|n|s| pad ax|s| pad r|eh|l|ax|t|ih|v|z|

Since I will split the data into individual phonemes anyway and I do
not pay any attention to the word boundaries, I replace the phoneme
seperators by spaces:

  #+begin_src python
train_ref = list(map(lambda st: str.replace(st, "|", " "), train_ref))
train_asr = list(map(lambda st: str.replace(st, "|", " "), train_asr))
train_ref[0]
  #+end_src

  #+RESULTS:
  : hh aw s  pad ah v  pad d eh r ey ax n  pad b ih k ey m  pad n ow n  pad th r uw  pad b ey ax n s  pad ae n d  pad w ih ch  pad ah v  pad b ey ax n s  pad ax s  pad r eh l ax t ih v z

This means I can now split the sentences into sequences of phonemes
and padding symbols:

  #+begin_src python
train_ref = [sentence.split() for sentence in train_ref]
train_asr = [sentence.split() for sentence in train_asr]
train_ref[0]
  #+end_src

  #+RESULTS:
  | hh | aw | s | pad | ah | v | pad | d | eh | r | ey | ax | n | pad | b | ih | k | ey | m | pad | n | ow | n | pad | th | r | uw | pad | b | ey | ax | n | s | pad | ae | n | d | pad | w | ih | ch | pad | ah | v | pad | b | ey | ax | n | s | pad | ax | s | pad | r | eh | l | ax | t | ih | v | z |

** Dict
First I load in the dictionary data, which is already represented as phonemes:

  #+begin_src python
with open("data/cmu_lst1.pkl", "rb") as infile:
    lst1 = pickle.load(infile)

with open("data/cmu_lst2.pkl", "rb") as infile:
    lst2 = pickle.load(infile)

(lst1[0], lst2[0])
  #+end_src

  #+RESULTS:
  | S | EH | M | IY | K | OW | L | AH | N |
  | S | EH | M | IH | K | OW | L | AH | N |

To better match the SQuAD and Subj datasets, I lowercase the phonemes:

  #+begin_src python
for i in range(len(lst1)):
    lst1[i] = list(map(lambda x: x.lower(), lst1[i]))
    lst2[i] = list(map(lambda x: x.lower(), lst2[i]))

(lst1[0], lst2[0])
  #+end_src

  #+RESULTS:
  | s | eh | m | iy | k | ow | l | ah | n |
  | s | eh | m | ih | k | ow | l | ah | n |

I am also going to experiment with adding 'pad' padding symbol at the
end of each word, since I did that for the SQuAD and Subj datasets. I
am interested to see how this will affect the performance of the
phoneme embeddings.

  #+begin_src python
lst1_pad = copy.deepcopy(lst1)
lst2_pad = copy.deepcopy(lst2)
for i in range(len(lst1_pad)):
    lst1_pad[i].append('pad')
    lst2_pad[i].append('pad')

(lst1_pad[0], lst2_pad[0])
  #+end_src

  #+RESULTS:
  | s | eh | m | iy | k | ow | l | ah | n | pad |
  | s | eh | m | ih | k | ow | l | ah | n | pad |

Now the non-padded and padded Dict data is ready to use for training
phoneme embeddings!

* s2s
** Functions / Classes
The code used for training the seq2seq model was adapted from [[https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html][PyTorch]].

   #+begin_src python :results silent
SOS_token = 0
EOS_token = 1

class Lang:
    def __init__(self, name):
        self.name = name
        self.phoneme2index = {}
        self.phoneme2count = {}
        self.index2phoneme = {0: "SOS", 1: "EOS"}
        self.n_phonemes = 2

    def addPhonemeList(self, phoneme_list):
        for phoneme in phoneme_list:
            if phoneme not in self.phoneme2index:
                self.phoneme2index[phoneme] = self.n_phonemes
                self.phoneme2count[phoneme] = 1
                self.index2phoneme[self.n_phonemes] = phoneme
                self.n_phonemes += 1

            else:
                self.phoneme2count[phoneme] += 1
   #+end_src

  #+begin_src python :results silent
class EncoderRNN(nn.Module):
    def __init__(self, input_size, hidden_size):
        super(EncoderRNN, self).__init__()
        self.hidden_size = hidden_size
        self.embedding = nn.Embedding(input_size, hidden_size)
        self.gru = nn.GRU(hidden_size, hidden_size)

    def forward(self, input, hidden):
        embedded = self.embedding(input).view(1, 1, -1)
        output = embedded
        output, hidden = self.gru(output, hidden)
        return output, hidden

    def initHidden(self):
        return torch.zeros(1, 1, self.hidden_size, device=device)
  #+end_src

  #+begin_src python :results silent
class DecoderRNN(nn.Module):
    def __init__(self, hidden_size, output_size):
        super(DecoderRNN, self).__init__()
        self.hidden_size = hidden_size
        self.embedding = nn.Embedding(output_size, hidden_size)
        self.gru = nn.GRU(hidden_size, hidden_size)
        self.out = nn.Linear(hidden_size, output_size)
        self.softmax = nn.Softmax(dim=1)

    def forward(self, input, hidden):
        output = self.embedding(input).view(1, 1, -1)
        output = F.relu(output)
        output, hidden = self.gru(output, hidden)
        output = self.softmax(self.out(output[0]))
        return output, hidden

    def initHidden(self):
        return torch.zeros(1, 1, self.hidden_size, device=device)
  #+end_src

  #+begin_src python :results silent
def indexesFromSentence(lang, sentence):
    return [lang.phoneme2index[phoneme] for phoneme in sentence]


def tensorFromSentence(lang, sentence):
    indexes = indexesFromSentence(lang, sentence)
    indexes.append(EOS_token)
    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)


def tensorsFromPair(pair):
    input_tensor = tensorFromSentence(input_lang, pair[0])
    target_tensor = tensorFromSentence(output_lang, pair[1])
    return (input_tensor, target_tensor)
  #+end_src

  #+begin_src python :results silent
def train(
    input_tensor,
    target_tensor,
    encoder,
    decoder,
    encoder_optimizer,
    decoder_optimizer,
    criterion,
    max_length=MAX_LENGTH,
):
    encoder_hidden = encoder.initHidden()
    encoder_optimizer.zero_grad()
    decoder_optimizer.zero_grad()
    input_length = input_tensor.size(0)
    target_length = target_tensor.size(0)
    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)
    loss = 0
    for ei in range(input_length):
        encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)
        encoder_outputs[ei] = encoder_output[0, 0]

    decoder_input = torch.tensor([[SOS_token]], device=device)
    decoder_hidden = encoder_hidden
    for di in range(target_length):
        decoder_output, decoder_hidden = decoder(
            decoder_input, decoder_hidden
        )
        topv, topi = decoder_output.topk(1)
        decoder_input = topi.squeeze().detach()  # detach from history as input
        loss += criterion(decoder_output, target_tensor[di])
        if decoder_input.item() == EOS_token:
            break

    loss.backward()
    encoder_optimizer.step()
    decoder_optimizer.step()
    return loss.item() / target_length
  #+end_src

  #+begin_src python :results silent
def asMinutes(s):
    m = math.floor(s / 60)
    s -= m * 60
    return '%dm %ds' % (m, s)


def timeSince(since, percent):
    now = time.time()
    s = now - since
    es = s / (percent)
    rs = es - s
    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))
  #+end_src

  #+begin_src python :results silent
def trainIters(
    encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01
):
    start = time.time()
    plot_losses = []
    print_loss_total = 0  # Reset every print_every
    plot_loss_total = 0  # Reset every plot_every
    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)
    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)
    training_pairs = [tensorsFromPair(random.choice(pairs)) for i in range(n_iters)]
    criterion = nn.CrossEntropyLoss()
    for iter in range(1, n_iters + 1):
        training_pair = training_pairs[iter - 1]
        input_tensor = training_pair[0]
        target_tensor = training_pair[1]
        loss = train(
            input_tensor,
            target_tensor,
            encoder,
            decoder,
            encoder_optimizer,
            decoder_optimizer,
            criterion,
        )
        print_loss_total += loss
        plot_loss_total += loss

        if iter % print_every == 0:
            print_loss_avg = print_loss_total / print_every
            print_loss_total = 0
            print(
                "%s (%d %d%%) %.4f"
                % (
                    timeSince(start, iter / n_iters),
                    iter,
                    iter / n_iters * 100,
                    print_loss_avg,
                )
            )

        if iter % plot_every == 0:
            plot_loss_avg = plot_loss_total / plot_every
            plot_losses.append(plot_loss_avg)
            plot_loss_total = 0
  #+end_src

   #+begin_src python :results silent
def annotate(row, ax):
    ax.annotate(row.name, (row.x, row.y),
                xytext=(10, -5), textcoords='offset points')
   #+end_src

** SQuAD and Subj
I first define the max length of a sentence, which also sets the
length for the encoder outputs.

  #+begin_src python :results silent
MAX_LENGTH = max(
    max(map(lambda x: len(x), train_ref)), max(map(lambda x: len(x), train_asr))
) + 1
  #+end_src

This function that builds up a language:

  #+begin_src python :results silent
def readLangs(lang1, lang2):
    pairs = [[train_ref[i], train_asr[i]] for i in range(len(train_ref))]
    input_lang = Lang(lang1)
    output_lang = Lang(lang2)
    return input_lang, output_lang, pairs
  #+end_src

Is used to build up the input (REF) and output (ASR) languages:

  #+begin_src python
input_lang, output_lang, pairs = readLangs("ref", "asr")
for pair in pairs:
    input_lang.addPhonemeList(pair[0])
    output_lang.addPhonemeList(pair[1])

(input_lang.name, input_lang.n_phonemes, output_lang.name, output_lang.n_phonemes)
  #+end_src

  #+RESULTS:
  | ref | 43 | asr | 43 |

So far, this model has been trained for 3 * 75000 iterations.

  #+begin_src python :results silent
hidden_size = 20
encoder1 = EncoderRNN(input_lang.n_phonemes, hidden_size).to(device)
decoder1 = DecoderRNN(hidden_size, output_lang.n_phonemes).to(device)
  #+end_src

  #+begin_src python :results silent
with open("encoder_asr.pkl", "rb") as input_file:
     encoder1 = pickle.load(input_file)

with open("decoder_asr.pkl", "rb") as input_file:
     decoder1 = pickle.load(input_file)
  #+end_src

  #+begin_src python
trainIters(encoder1, decoder1, 75000, print_every=5000)

with open("encoder_asr.pkl", "wb") as output_file:
     pickle.dump(encoder1, output_file)

with open("decoder_asr.pkl", "wb") as output_file:
     pickle.dump(decoder1, output_file)
  #+end_src

*** Visualization
To create a t-SNE plot, I need all the phonemes included in the
embedding, as well as the embedding itself:

   #+begin_src python
phonemes = list(input_lang.phoneme2index.keys())
vocab = dict([(x,y) for (y,x) in enumerate(phonemes)])
vocab_dict = {"vocab": vocab, "rev": phonemes}
with open("models/s2s_asr_vocab.pkl", "wb") as outfile:
    pickle.dump(vocab_dict, outfile)
X = decoder1.embedding(torch.LongTensor(np.arange(2, 43)))
#To create a numpy array out of the tensor:
X = X.detach().numpy()
np.save("models/s2s_asr.npy", X)
(phonemes, X.shape)
   #+end_src

   #+RESULTS:
   | hh | aw | s | pad | ah | v | d | eh | r | ey | ax | n | b | ih | k | m | ow | th | uw | ae | w | ch | l | t | z | aa | f | ao | er | p | sh | ng | ay | uh | y | iy | g | dh | jh | oy | zh |
   | 41 | 20 |   |     |    |   |   |    |   |    |    |   |   |    |   |   |    |    |    |    |   |    |   |   |   |    |   |    |    |   |    |    |    |    |   |    |   |    |    |    |    |

Now I can fit the t-SNE and put the results in a DataFrame:

   #+begin_src python
tsne = TSNE(n_components=2)
X_tsne = tsne.fit_transform(X)
df = pd.DataFrame(X_tsne, index=phonemes, columns=["x", "y"])

df.shape
   #+end_src

   #+RESULTS:
   | 41 | 2 |

Now the t-SNE can be plotted:

   #+begin_src python :results silent
ax1 = df.plot.scatter(x="x", y="y")
df.apply(annotate, ax=ax1, axis=1)
fig = ax1.get_figure()
fig.savefig("figures/s2s_asr.png")
   #+end_src

** Dict
I first define the max length of a sentence, which also sets the
length for the encoder outputs.

  #+begin_src python :results silent
MAX_LENGTH = max(
    max(map(lambda x: len(x), lst1)), max(map(lambda x: len(x), lst2))
) + 1
  #+end_src

This function that builds up a language:

  #+begin_src python :results silent
def readLangs(lang1, lang2):
    pairs = [[lst1[i], lst2[i]] for i in range(len(lst1))]
    input_lang = Lang(lang1)
    output_lang = Lang(lang2)
    return input_lang, output_lang, pairs
  #+end_src

Is used to build up the input (lst1) and output (lst2) languages:

  #+begin_src python
input_lang, output_lang, pairs = readLangs("lst1", "lst2")
for pair in pairs:
    input_lang.addPhonemeList(pair[0])
    output_lang.addPhonemeList(pair[1])

(input_lang.name, input_lang.n_phonemes, output_lang.name, output_lang.n_phonemes)
  #+end_src

  #+RESULTS:
  | lst1 | 41 | lst2 | 41 |

So far, this model has been trained for 3 * 75000 iterations.

  #+begin_src python :results silent
hidden_size = 20
encoder1 = EncoderRNN(input_lang.n_phonemes, hidden_size).to(device)
decoder1 = DecoderRNN(hidden_size, output_lang.n_phonemes).to(device)
  #+end_src

  #+begin_src python :results silent
with open("encoder_dict.pkl", "rb") as input_file:
     encoder1 = pickle.load(input_file)

with open("decoder_dict.pkl", "rb") as input_file:
     decoder1 = pickle.load(input_file)
  #+end_src

  #+begin_src python
trainIters(encoder1, decoder1, 75000, print_every=5000)

with open("encoder_dict.pkl", "wb") as output_file:
     pickle.dump(encoder1, output_file)

with open("decoder_dict.pkl", "wb") as output_file:
     pickle.dump(decoder1, output_file)
  #+end_src

*** Visualization
To create a t-SNE plot, I need all the phonemes included in the
embedding, as well as the embedding itself. The Dict data has one less
phoneme compared to the phonemized SQuAD and Subj data, namely the
'ax' phoneme:

   #+begin_src python
phonemes = list(input_lang.phoneme2index.keys())
vocab = dict([(x,y) for (y,x) in enumerate(phonemes + ["ax"])])
vocab_dict = {"vocab": vocab, "rev": phonemes + ["ax"]}
with open("models/s2s_dict_vocab.pkl", "wb") as outfile:
    pickle.dump(vocab_dict, outfile)
X = decoder1.embedding(torch.LongTensor(np.arange(2, 41)))
#To create a numpy array out of the tensor:
X = X.detach().numpy()
idx = phonemes.index("k")
X = np.concatenate([X, X[idx].reshape((1, 20))])
np.save("models/s2s_dict.npy", X)

(phonemes, X.shape)
   #+end_src

   #+RESULTS:
   |  s | eh | m | iy | k | ow | l | ah | n | r | z | b | aa | ae | uw | d | t | ih | ng | sh | er | y | ey | ao | v | p | ch | g | aw | w | ay | jh | hh | f | th | uh | oy | dh | zh |
   | 40 | 20 |   |    |   |    |   |    |   |   |   |   |    |    |    |   |   |    |    |    |    |   |    |    |   |   |    |   |    |   |    |    |    |   |    |    |    |    |    |

Now I can fit the t-SNE and put the results in a DataFrame:

   #+begin_src python
tsne = TSNE(n_components=2)
X_tsne = tsne.fit_transform(X)
df = pd.DataFrame(X_tsne, index=phonemes, columns=["x", "y"])

df.shape
   #+end_src

   #+RESULTS:
   | 39 | 2 |

Now the t-SNE can be plotted:

   #+begin_src python :results silent
ax1 = df.plot.scatter(x="x", y="y")
df.apply(annotate, ax=ax1, axis=1)
fig = ax1.get_figure()
fig.savefig("figures/s2s_dict.png")
   #+end_src

** Dict_pad
I first define the max length of a sentence, which also sets the
length for the encoder outputs.

  #+begin_src python :results silent
MAX_LENGTH = max(
    max(map(lambda x: len(x), lst1_pad)), max(map(lambda x: len(x), lst2_pad))
) + 1
  #+end_src

This function that builds up a language:

  #+begin_src python :results silent
def readLangs(lang1, lang2):
    pairs = [[lst1_pad[i], lst2_pad[i]] for i in range(len(lst1_pad))]
    input_lang = Lang(lang1)
    output_lang = Lang(lang2)
    return input_lang, output_lang, pairs
  #+end_src

Is used to build up the input (lst1_pad) and output (lst2_pad) languages:

  #+begin_src python
input_lang, output_lang, pairs = readLangs("lst1_pad", "lst2_pad")
for pair in pairs:
    input_lang.addPhonemeList(pair[0])
    output_lang.addPhonemeList(pair[1])

(input_lang.name, input_lang.n_phonemes, output_lang.name, output_lang.n_phonemes)
  #+end_src

  #+RESULTS:
  | lst1_pad | 42 | lst2_pad | 42 |

So far, this model has been trained for 3 * 75000 iterations.

  #+begin_src python :results silent
hidden_size = 20
encoder1 = EncoderRNN(input_lang.n_phonemes, hidden_size).to(device)
decoder1 = DecoderRNN(hidden_size, output_lang.n_phonemes).to(device)
  #+end_src

  #+begin_src python :results silent
with open("encoder_dict_pad.pkl", "rb") as input_file:
     encoder1 = pickle.load(input_file)

with open("decoder_dict_pad.pkl", "rb") as input_file:
     decoder1 = pickle.load(input_file)
  #+end_src

  #+begin_src python
trainIters(encoder1, decoder1, 75000, print_every=5000)

with open("encoder_dict_pad.pkl", "wb") as output_file:
     pickle.dump(encoder1, output_file)

with open("decoder_dict_pad.pkl", "wb") as output_file:
     pickle.dump(decoder1, output_file)
  #+end_src

*** Visualization
To create a t-SNE plot, I need all the phonemes included in the
embedding, as well as the embedding itself. The Dict data has one less
phoneme compared to the phonemized SQuAD and Subj data, namely the
'ax' phoneme:

   #+begin_src python
phonemes = list(input_lang.phoneme2index.keys())
vocab = dict([(x,y) for (y,x) in enumerate(phonemes + ["ax"])])
vocab_dict = {"vocab": vocab, "rev": phonemes + ["ax"]}
with open("models/s2s_dict_pad_vocab.pkl", "wb") as outfile:
    pickle.dump(vocab_dict, outfile)
X = decoder1.embedding(torch.LongTensor(np.arange(2, 42)))
#To create a numpy array out of the tensor:
X = X.detach().numpy()
idx = phonemes.index("k")
X = np.concatenate([X, X[idx].reshape((1, 20))])
np.save("models/s2s_dict_pad.npy", X)

(phonemes, X.shape)
   #+end_src

   #+RESULTS:
   |  s | eh | m | iy | k | ow | l | ah | n | pad | r | z | b | aa | ae | uw | d | t | ih | ng | sh | er | y | ey | ao | v | p | ch | g | aw | w | ay | jh | hh | f | th | uh | oy | dh | zh |
   | 41 | 20 |   |    |   |    |   |    |   |     |   |   |   |    |    |    |   |   |    |    |    |    |   |    |    |   |   |    |   |    |   |    |    |    |   |    |    |    |    |    |

Now I can fit the t-SNE and put the results in a DataFrame:

   #+begin_src python
tsne = TSNE(n_components=2)
X_tsne = tsne.fit_transform(X)
df = pd.DataFrame(X_tsne, index=phonemes, columns=["x", "y"])

df.shape
   #+end_src

   #+RESULTS:
   | 40 | 2 |

Now the t-SNE can be plotted:

   #+begin_src python :results silent
ax1 = df.plot.scatter(x="x", y="y")
df.apply(annotate, ax=ax1, axis=1)
fig = ax1.get_figure()
fig.savefig("figures/s2s_dict_pad.png")
   #+end_src
