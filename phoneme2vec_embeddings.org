#+TITLE: Train Phoneme2Vec phoneme embeddings
#+AUTHOR: Vera Neplenbroek
#+DATE: Wednesday, 17 February 2021
#+PROPERTY: header-args :exports both :session phoneme_emb :cache no :results value

* Imports
  #+begin_src python :results silent
from collections import deque
import copy
from itertools import chain, product, zip_longest
import multiprocessing

from gensim.models import Word2Vec
import gensim.models.word2vec
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
import numpy as np
import pandas as pd
import pickle
from pylab import figure
from sklearn.manifold import TSNE
import tqdm
  #+end_src

* Data
** SQuAD and Subj
First I load in the training data in their phonemized versions:

  #+begin_src python :results silent
with open(
    "data/phoneme_embedding_training_set/squad_samples_reference.phon", "r"
) as infile:
    squad_ref_phon = [i.strip() for i in infile.readlines()]
with open("data/phoneme_embedding_training_set/squad_samples_asr.phon", "r") as infile:
    squad_asr_phon = [i.strip() for i in infile.readlines()]
with open(
    "data/phoneme_embedding_training_set/subj_samples_reference.phon", "r"
) as infile:
    subj_ref_phon = [i.strip() for i in infile.readlines()]
with open("data/phoneme_embedding_training_set/subj_samples_asr.phon", "r") as infile:
    subj_asr_phon = [i.strip() for i in infile.readlines()]
  #+end_src

Now let's have a look at the reference and asr versions of the two
datasets:

  #+begin_src python
squad_ref_phon[0]
  #+end_src

  #+RESULTS:
  : hh|aw|s| ah|v| d|eh|r|ey|ax|n| b|ih|k|ey|m| n|ow|n| th|r|uw| b|ey|ax|n|s| ae|n|d| w|ih|ch| ah|v| b|ey|ax|n|s| ax|s| r|eh|l|ax|t|ih|v|z|

  #+begin_src python
squad_asr_phon[0]
  #+end_src

  #+RESULTS:
  : hh|aw|s| ah|v| d|eh|r|ey|ax|n| b|ih|k|ey|m| n|ow|n| th|r|uw| b|ey|ax|n|s| ae|n|d| w|ih|ch| ah|v| b|ih|aa|n|d| s|ey| eh|s| r|eh|l|ax|t|ih|v|z|

  #+begin_src python
subj_ref_phon[0]
  #+end_src

  #+RESULTS:
  : dh|ax| m|uw|v|iy| b|ax|g|ih|n|z| ih|n| dh|ax| p|ae|s|t| w|eh|r| ax| y|ah|ng| b|oy| n|ey|m|d| s|ae|m| ax|t|eh|m|p|t|s| t|ax| s|ey|v| s|eh|l|ey|b|iy| f|r|ah|m| ax| hh|ah|n|t|er|

  #+begin_src python
subj_asr_phon[0]
  #+end_src

  #+RESULTS:
  : dh|ax| m|uw|v|iy| b|ax|g|ih|n|z| ih|n| dh|ax| p|ae|s|t| w|eh|r| ax| y|ah|ng| b|oy| n|ey|m|d| s|ae|m| ax|t|eh|m|p|t|s| t|ax| s|ey|v| s|eh|l|ax|b| iy| f|r|ah|m| dh|ax| hh|ah|n|t|er|

Now I will add the two datasets together, since I will use a mix of
both for training. It is important that the order of the lists within
the lists, corresponding to sentences, stays the same.

  #+begin_src python
train_ref = squad_ref_phon + subj_ref_phon
train_asr = squad_asr_phon + subj_asr_phon
(len(train_ref), len(train_asr))
  #+end_src

  #+RESULTS:
  | 14715 | 14715 |

I put the padding symbol 'pad' in between two words:

  #+begin_src python
train_ref = list(map(lambda st: str.replace(st, " ", " pad "), train_ref))
train_asr = list(map(lambda st: str.replace(st, " ", " pad "), train_asr))
train_ref[0]
  #+end_src

  #+RESULTS:
  : hh|aw|s| pad ah|v| pad d|eh|r|ey|ax|n| pad b|ih|k|ey|m| pad n|ow|n| pad th|r|uw| pad b|ey|ax|n|s| pad ae|n|d| pad w|ih|ch| pad ah|v| pad b|ey|ax|n|s| pad ax|s| pad r|eh|l|ax|t|ih|v|z|

Since I will split the data into individual phonemes anyway and I do
not pay any attention to the word boundaries, I replace the phoneme
seperators by spaces:

  #+begin_src python
train_ref = list(map(lambda st: str.replace(st, "|", " "), train_ref))
train_asr = list(map(lambda st: str.replace(st, "|", " "), train_asr))
train_ref[0]
  #+end_src

  #+RESULTS:
  : hh aw s  pad ah v  pad d eh r ey ax n  pad b ih k ey m  pad n ow n  pad th r uw  pad b ey ax n s  pad ae n d  pad w ih ch  pad ah v  pad b ey ax n s  pad ax s  pad r eh l ax t ih v z

This means I can now split the sentences into sequences of phonemes
and padding symbols:

  #+begin_src python
train_ref = [sentence.split() for sentence in train_ref]
train_asr = [sentence.split() for sentence in train_asr]
train_ref[0]
  #+end_src

  #+RESULTS:
  | hh | aw | s | pad | ah | v | pad | d | eh | r | ey | ax | n | pad | b | ih | k | ey | m | pad | n | ow | n | pad | th | r | uw | pad | b | ey | ax | n | s | pad | ae | n | d | pad | w | ih | ch | pad | ah | v | pad | b | ey | ax | n | s | pad | ax | s | pad | r | eh | l | ax | t | ih | v | z |

** Dict
First I load in the dictionary data, which is already represented as phonemes:

  #+begin_src python
with open("data/cmu_lst1.pkl", "rb") as infile:
    lst1 = pickle.load(infile)

with open("data/cmu_lst2.pkl", "rb") as infile:
    lst2 = pickle.load(infile)

(lst1[0], lst2[0])
  #+end_src

  #+RESULTS:
  | S | EH | M | IY | K | OW | L | AH | N |
  | S | EH | M | IH | K | OW | L | AH | N |

To better match the SQuAD and Subj datasets, I lowercase the phonemes:

  #+begin_src python
for i in range(len(lst1)):
    lst1[i] = list(map(lambda x: x.lower(), lst1[i]))
    lst2[i] = list(map(lambda x: x.lower(), lst2[i]))

(lst1[0], lst2[0])
  #+end_src

  #+RESULTS:
  | s | eh | m | iy | k | ow | l | ah | n |
  | s | eh | m | ih | k | ow | l | ah | n |

I am also going to experiment with adding 'pad' padding symbol at the
end of each word, since I did that for the SQuAD and Subj datasets. I
am interested to see how this will affect the performance of the
phoneme embeddings.

  #+begin_src python
lst1_pad = copy.deepcopy(lst1)
lst2_pad = copy.deepcopy(lst2)
for i in range(len(lst1_pad)):
    lst1_pad[i].append('pad')
    lst2_pad[i].append('pad')

(lst1_pad[0], lst2_pad[0])
  #+end_src

  #+RESULTS:
  | s | eh | m | iy | k | ow | l | ah | n | pad |
  | s | eh | m | ih | k | ow | l | ah | n | pad |

Now the non-padded and padded Dict data is ready to use for training
phoneme embeddings!

* p2vc
** SQuAD and Subj
*** Embedding
For the first embedding I can directly train it and the reference and
asr sets are just added together:

  #+begin_src python :results silent
p2vc = Word2Vec.load("models/p2vc_asr.model")
  #+end_src

  #+begin_src python :results silent
EMB_DIM = 20
p2vc = Word2Vec(
    negative=30,
    sentences=train_ref + train_asr,
    size=EMB_DIM,
    window=2,
    sg=1,
    iter=10,
    workers=multiprocessing.cpu_count(),
)
p2vc.save("models/p2vc_asr.model")
  #+end_src

These are the similarity scores for phonemes I expect to be similar,
based on intuition:

  #+begin_src python
(
    p2vc.wv.similarity("ay", "oy"),
    p2vc.wv.similarity("ey", "ay"),
    p2vc.wv.similarity("uw", "aw"),
    p2vc.wv.similarity("sh", "zh"),
)
  #+end_src

  #+RESULTS:
  | 0.58399534 | 0.49644744 | 0.38648778 | 0.52161956 |

These are the similarity scores for phonemes I expect to be
dissimilar, based on intuition:

  #+begin_src python
(
    p2vc.wv.similarity("hh", "oy"),
    p2vc.wv.similarity("v", "dh"),
    p2vc.wv.similarity("z", "th"),
    p2vc.wv.similarity("w", "l"),
)
  #+end_src

  #+RESULTS:
  | 0.24047394 | 0.46784192 | 0.37347177 | 0.23343764 |

It looks like the model gives somewhat higher scores to similar
sounding phonemes, but the difference is not very big.

*** Visualization
To create a t-SNE plot, I need all the phonemes included in the
embedding, as well as the embedding itself:

   #+begin_src python
phonemes = list(p2vc.wv.vocab)
vocab = dict([(x,y) for (y,x) in enumerate(phonemes)])
vocab_dict = {"vocab": vocab, "rev": phonemes}
with open("models/p2vc_asr_vocab.pkl", "wb") as outfile:
    pickle.dump(vocab_dict, outfile)
X = p2vc[phonemes]
np.save("models/p2vc_asr.npy", X)

(phonemes, X.shape)
   #+end_src

   #+RESULTS:
   | hh | aw | s | pad | ah | v | d | eh | r | ey | ax | n | b | ih | k | m | ow | th | uw | ae | w | ch | l | t | z | aa | f | ao | er | p | sh | ng | ay | uh | y | iy | g | dh | jh | oy | zh |
   | 41 | 20 |   |     |    |   |   |    |   |    |    |   |   |    |   |   |    |    |    |    |   |    |   |   |   |    |   |    |    |   |    |    |    |    |   |    |   |    |    |    |    |

Now I can fit the t-SNE and put the results in a DataFrame:

   #+begin_src python
tsne = TSNE(n_components=2)
X_tsne = tsne.fit_transform(X)
df = pd.DataFrame(X_tsne, index=phonemes, columns=["x", "y"])

df.shape
   #+end_src

   #+RESULTS:
   | 41 | 2 |

   #+begin_src python :results silent
def annotate(row, ax):
    ax.annotate(row.name, (row.x, row.y),
                xytext=(10, -5), textcoords='offset points')
   #+end_src

Now the t-SNE can be plotted:

   #+begin_src python :results silent
ax1 = df.plot.scatter(x="x", y="y")
df.apply(annotate, ax=ax1, axis=1)
fig = ax1.get_figure()
fig.savefig("figures/p2vc_asr.png")
   #+end_src

** Dict
*** Embedding
For the first embedding I can directly train it and lst1 and lst2 are
just added together:

  #+begin_src python :results silent
p2vc = Word2Vec.load("models/p2vc_dict.model")
  #+end_src

  #+begin_src python :results silent
EMB_DIM = 20
p2vc = Word2Vec(
    negative=30,
    sentences=lst1+lst2,
    size=EMB_DIM,
    window=2,
    sg=1,
    iter=10,
    workers=multiprocessing.cpu_count(),
)

p2vc.save("models/p2vc_dict.model")
  #+end_src

These are the similarity scores for phonemes I expect to be similar,
based on intuition:

  #+begin_src python
(
    p2vc.wv.similarity("ay", "oy"),
    p2vc.wv.similarity("ey", "ay"),
    p2vc.wv.similarity("uw", "aw"),
    p2vc.wv.similarity("sh", "zh"),
)
  #+end_src

  #+RESULTS:
  | 0.7341004 | 0.66853726 | 0.47444394 | 0.7107227 |

These are the similarity scores for phonemes I expect to be
dissimilar, based on intuition:

  #+begin_src python
(
    p2vc.wv.similarity("hh", "oy"),
    p2vc.wv.similarity("v", "dh"),
    p2vc.wv.similarity("z", "th"),
    p2vc.wv.similarity("w", "l"),
)
  #+end_src

  #+RESULTS:
  | 0.3852948 | 0.49625322 | 0.6442277 | 0.49043572 |

It looks like the model gives somewhat higher scores to similar
sounding phonemes, but the difference is not very big, except for "hh"
and "oy".

*** Visualization
To create a t-SNE plot, I need all the phonemes included in the
embedding, as well as the embedding itself. The Dict data has one less
phoneme compared to the phonemized SQuAD and Subj data, namely the
'ax' phoneme:

   #+begin_src python
phonemes = list(p2vc.wv.vocab)
vocab = dict([(x,y) for (y,x) in enumerate(phonemes + ["ax"])])
vocab_dict = {"vocab": vocab, "rev": phonemes + ["ax"]}
with open("models/p2vc_dict_vocab.pkl", "wb") as outfile:
    pickle.dump(vocab_dict, outfile)
idx = phonemes.index("er")
X = np.concatenate([p2vc[phonemes], p2vc[phonemes][idx].reshape((1, 20))])
np.save("models/p2vc_dict.npy", X)

(phonemes, X.shape)
   #+end_src

   #+RESULTS:
   |  s | eh | m | iy | k | ow | l | ah | n | r | z | b | aa | ae | uw | d | t | ih | ng | sh | er | y | ey | ao | v | p | ch | g | aw | w | ay | jh | hh | f | th | uh | oy | dh | zh |
   | 40 | 20 |   |    |   |    |   |    |   |   |   |   |    |    |    |   |   |    |    |    |    |   |    |    |   |   |    |   |    |   |    |    |    |   |    |    |    |    |    |

Now I can fit the t-SNE and put the results in a DataFrame:

   #+begin_src python
tsne = TSNE(n_components=2)
X_tsne = tsne.fit_transform(X)
df = pd.DataFrame(X_tsne, index=phonemes, columns=["x", "y"])

df.shape
   #+end_src

   #+RESULTS:
   | 39 | 2 |

   #+begin_src python :results silent
def annotate(row, ax):
    ax.annotate(row.name, (row.x, row.y),
                xytext=(10, -5), textcoords='offset points')
   #+end_src

Now the t-SNE can be plotted:

   #+begin_src python :results silent
ax1 = df.plot.scatter(x="x", y="y")
df.apply(annotate, ax=ax1, axis=1)
fig = ax1.get_figure()
fig.savefig("figures/p2vc_dict.png")
   #+end_src

** Dict_pad
*** Embedding
For the first embedding I can directly train it and lst1_pad and lst2_pad are
just added together:

  #+begin_src python :results silent
p2vc = Word2Vec.load("models/p2vc_dict_pad.model")
  #+end_src

  #+begin_src python :results silent
EMB_DIM = 20
p2vc = Word2Vec(
    negative=30,
    sentences=lst1_pad+lst2_pad,
    size=EMB_DIM,
    window=2,
    sg=1,
    iter=10,
    workers=multiprocessing.cpu_count(),
)
p2vc.save("models/p2vc_dict_pad.model")
  #+end_src

These are the similarity scores for phonemes I expect to be similar,
based on intuition:

  #+begin_src python
(
    p2vc.wv.similarity("ay", "oy"),
    p2vc.wv.similarity("ey", "ay"),
    p2vc.wv.similarity("uw", "aw"),
    p2vc.wv.similarity("sh", "zh"),
)
  #+end_src

  #+RESULTS:
  | 0.6966101 | 0.64497685 | 0.48431978 | 0.60999256 |

These are the similarity scores for phonemes I expect to be
dissimilar, based on intuition:

  #+begin_src python
(
    p2vc.wv.similarity("hh", "oy"),
    p2vc.wv.similarity("v", "dh"),
    p2vc.wv.similarity("z", "th"),
    p2vc.wv.similarity("w", "l"),
)
  #+end_src

  #+RESULTS:
  | 0.3262307 | 0.42761162 | 0.6622027 | 0.41843978 |

It looks like the model gives somewhat higher scores to similar
sounding phonemes, but the difference is not very big, except for "hh"
and "oy".

*** Visualization
To create a t-SNE plot, I need all the phonemes included in the
embedding, as well as the embedding itself. The Dict data has one less
phoneme compared to the phonemized SQuAD and Subj data, namely the
'ax' phoneme:

   #+begin_src python
phonemes = list(p2vc.wv.vocab)
vocab = dict([(x,y) for (y,x) in enumerate(phonemes + ["ax"])])
vocab_dict = {"vocab": vocab, "rev": phonemes + ["ax"]}
with open("models/p2vc_dict_pad_vocab.pkl", "wb") as outfile:
    pickle.dump(vocab_dict, outfile)
idx = phonemes.index("er")
X = np.concatenate([p2vc[phonemes], p2vc[phonemes][idx].reshape((1, 20))])
np.save("models/p2vc_dict_pad.npy", X)

(phonemes, X.shape)
   #+end_src

   #+RESULTS:
   |  s | eh | m | iy | k | ow | l | ah | n | pad | r | z | b | aa | ae | uw | d | t | ih | ng | sh | er | y | ey | ao | v | p | ch | g | aw | w | ay | jh | hh | f | th | uh | oy | dh | zh |
   | 41 | 20 |   |    |   |    |   |    |   |     |   |   |   |    |    |    |   |   |    |    |    |    |   |    |    |   |   |    |   |    |   |    |    |    |   |    |    |    |    |    |

Now I can fit the t-SNE and put the results in a DataFrame:

   #+begin_src python
tsne = TSNE(n_components=2)
X_tsne = tsne.fit_transform(X)
df = pd.DataFrame(X_tsne, index=phonemes, columns=["x", "y"])

df.shape
   #+end_src

   #+RESULTS:
   | 40 | 2 |

   #+begin_src python :results silent
def annotate(row, ax):
    ax.annotate(row.name, (row.x, row.y),
                xytext=(10, -5), textcoords='offset points')
   #+end_src

Now the t-SNE can be plotted:

   #+begin_src python :results silent
ax1 = df.plot.scatter(x="x", y="y")
df.apply(annotate, ax=ax1, axis=1)
fig = ax1.get_figure()
fig.savefig("figures/p2vc_dict_pad.png")
   #+end_src

* p2vm
** SQuAD and Subj
*** Embedding
For this embedding I first need to create a list of lists where the
inner lists are made up out of alternating elements (phonemes) from
the reference and asr sentences. If one list is longer than the other,
the 'extra' elements (phonemes) are added at the end of the mixed
list.

  #+begin_src python
train_mixed_p2vm = [
    list(filter(None, chain(*zip_longest(train_ref[i], train_asr[i]))))
    for i in range(len(train_ref))
]
train_mixed_p2vm[0]
  #+end_src

  #+RESULTS:
  | hh | hh | aw | aw | s | s | pad | pad | ah | ah | v | v | pad | pad | d | d | eh | eh | r | r | ey | ey | ax | ax | n | n | pad | pad | b | b | ih | ih | k | k | ey | ey | m | m | pad | pad | n | n | ow | ow | n | n | pad | pad | th | th | r | r | uw | uw | pad | pad | b | b | ey | ey | ax | ax | n | n | s | s | pad | pad | ae | ae | n | n | d | d | pad | pad | w | w | ih | ih | ch | ch | pad | pad | ah | ah | v | v | pad | pad | b | b | ey | ih | ax | aa | n | n | s | d | pad | pad | ax | s | s | ey | pad | pad | r | eh | eh | s | l | pad | ax | r | t | eh | ih | l | v | ax | z | t | ih | v | z |

Now I can train the embedding:

  #+begin_src python :results silent
p2vm = Word2Vec.load("models/p2vm_asr.model")
  #+end_src

  #+begin_src python :results silent
EMB_DIM = 20
p2vm = Word2Vec(
    negative=30,
    sentences=train_mixed_p2vm,
    size=EMB_DIM,
    window=2,
    sg=1,
    iter=10,
    workers=multiprocessing.cpu_count(),
)
p2vm.save("models/p2vm_asr.model")
  #+end_src

These are the similarity scores for phonemes I expect to be similar,
based on intuition:

  #+begin_src python
(
    p2vm.wv.similarity("ay", "oy"),
    p2vm.wv.similarity("ey", "ay"),
    p2vm.wv.similarity("uw", "aw"),
    p2vm.wv.similarity("sh", "zh"),
)
  #+end_src

  #+RESULTS:
  | 0.34669897 | 0.1448062 | 0.2830301 | 0.37006277 |

These are the similarity scores for phonemes I expect to be
dissimilar, based on intuition:

  #+begin_src python
(
    p2vm.wv.similarity("hh", "oy"),
    p2vm.wv.similarity("v", "dh"),
    p2vm.wv.similarity("z", "th"),
    p2vm.wv.similarity("w", "l"),
)
  #+end_src

  #+RESULTS:
  | 0.113394454 | 0.4738238 | 0.17297195 | 0.2014148 |

It looks like the model gives similar scores to similar sounding
phonemes and dissimilar sounding phonemes. Something that surprises me
is the very low score for "ey" and "ay", even though they are similar
sounding phonemes.
*** Visualization
To create a t-SNE plot, I need all the phonemes included in the
embedding, as well as the embedding itself:

   #+begin_src python
phonemes = list(p2vm.wv.vocab)
vocab = dict([(x,y) for (y,x) in enumerate(phonemes)])
vocab_dict = {"vocab": vocab, "rev": phonemes}
with open("models/p2vm_asr_vocab.pkl", "wb") as outfile:
    pickle.dump(vocab_dict, outfile)
X = p2vm[phonemes]
np.save("models/p2vm_asr.npy", X)

(phonemes, X.shape)
   #+end_src

   #+RESULTS:
   | hh | aw | s | pad | ah | v | d | eh | r | ey | ax | n | b | ih | k | m | ow | th | uw | ae | w | ch | aa | l | t | z | f | ao | er | p | sh | ay | ng | uh | y | dh | iy | g | jh | oy | zh |
   | 41 | 20 |   |     |    |   |   |    |   |    |    |   |   |    |   |   |    |    |    |    |   |    |    |   |   |   |   |    |    |   |    |    |    |    |   |    |    |   |    |    |    |

Now I can fit the t-SNE and put the results in a DataFrame:

   #+begin_src python
tsne = TSNE(n_components=2)
X_tsne = tsne.fit_transform(X)
df = pd.DataFrame(X_tsne, index=phonemes, columns=["x", "y"])

df.shape
   #+end_src

   #+RESULTS:
   | 41 | 2 |

   #+begin_src python :results silent
def annotate(row, ax):
    ax.annotate(row.name, (row.x, row.y),
                xytext=(10, -5), textcoords='offset points')
   #+end_src

Now the t-SNE can be plotted:

   #+begin_src python :results silent
ax1 = df.plot.scatter(x="x", y="y")
df.apply(annotate, ax=ax1, axis=1)
fig = ax1.get_figure()
fig.savefig("figures/p2vm_asr.png")
   #+end_src

** Dict
*** Embedding
For this embedding I first need to create a list of lists where the
inner lists are made up out of alternating elements (phonemes) from
lst1 and lst2. If one list is longer than the other, the 'extra'
elements (phonemes) are added at the end of the mixed list.

  #+begin_src python
train_mixed_p2vm = [
    list(filter(None, chain(*zip_longest(lst1[i], lst2[i]))))
    for i in range(len(lst1))
]
train_mixed_p2vm[0]
  #+end_src

  #+RESULTS:
  | s | s | eh | eh | m | m | iy | ih | k | k | ow | ow | l | l | ah | ah | n | n |

Now I can train the embedding:

  #+begin_src python :results silent
p2vm = Word2Vec.load("models/p2vm_dict.model")
  #+end_src

  #+begin_src python :results silent
EMB_DIM = 20
p2vm = Word2Vec(
    negative=30,
    sentences=train_mixed_p2vm,
    size=EMB_DIM,
    window=2,
    sg=1,
    iter=10,
    workers=multiprocessing.cpu_count(),
)
p2vm.save("models/p2vm_dict.model")
  #+end_src

These are the similarity scores for phonemes I expect to be similar,
based on intuition:

  #+begin_src python
(
    p2vm.wv.similarity("ay", "oy"),
    p2vm.wv.similarity("ey", "ay"),
    p2vm.wv.similarity("uw", "aw"),
    p2vm.wv.similarity("sh", "zh"),
)
  #+end_src

  #+RESULTS:
  | 0.2336486 | 0.41370505 | 0.47227412 | 0.30347314 |

These are the similarity scores for phonemes I expect to be
dissimilar, based on intuition:

  #+begin_src python
(
    p2vm.wv.similarity("hh", "oy"),
    p2vm.wv.similarity("v", "dh"),
    p2vm.wv.similarity("z", "th"),
    p2vm.wv.similarity("w", "l"),
)
  #+end_src

  #+RESULTS:
  | 0.14764439 | 0.18273796 | 0.4554945 | 0.48987275 |

It looks like the model gives similar scores to similar sounding
phonemes and dissimilar sounding phonemes. Something that surprises me
is the very low score for "sh" and "zh", even though they are similar
sounding phonemes. The high score for "w" and "l" is also surprising,
because I do not expect them to sound similar.

*** Visualization
To create a t-SNE plot, I need all the phonemes included in the
embedding, as well as the embedding itself. The Dict data has one less
phoneme compared to the phonemized SQuAD and Subj data, namely the
'ax' phoneme:

   #+begin_src python
phonemes = list(p2vm.wv.vocab)
vocab = dict([(x,y) for (y,x) in enumerate(phonemes + ["ax"])])
vocab_dict = {"vocab": vocab, "rev": phonemes + ["ax"]}
with open("models/p2vm_dict_vocab.pkl", "wb") as outfile:
    pickle.dump(vocab_dict, outfile)
idx = phonemes.index("er")
X = np.concatenate([p2vm[phonemes], p2vm[phonemes][idx].reshape((1, 20))])
np.save("models/p2vm_dict.npy", X)

(phonemes, X.shape)
   #+end_src

   #+RESULTS:
   |  s | eh | m | iy | ih | k | ow | l | ah | n | ey | aa | r | z | b | ae | uw | aw | d | t | ng | sh | er | y | ao | v | p | ch | uh | g | th | w | ay | jh | hh | f | oy | dh | zh |
   | 40 | 20 |   |    |    |   |    |   |    |   |    |    |   |   |   |    |    |    |   |   |    |    |    |   |    |   |   |    |    |   |    |   |    |    |    |   |    |    |    |

Now I can fit the t-SNE and put the results in a DataFrame:

   #+begin_src python
tsne = TSNE(n_components=2)
X_tsne = tsne.fit_transform(X)
df = pd.DataFrame(X_tsne, index=phonemes, columns=["x", "y"])

df.shape
   #+end_src

   #+RESULTS:
   | 39 | 2 |

   #+begin_src python :results silent
def annotate(row, ax):
    ax.annotate(row.name, (row.x, row.y),
                xytext=(10, -5), textcoords='offset points')
   #+end_src

Now the t-SNE can be plotted:

   #+begin_src python :results silent
ax1 = df.plot.scatter(x="x", y="y")
df.apply(annotate, ax=ax1, axis=1)
fig = ax1.get_figure()
fig.savefig("figures/p2vm_dict.png")
   #+end_src

** Dict_pad
*** Embedding
For this embedding I first need to create a list of lists where the
inner lists are made up out of alternating elements (phonemes) from
lst1_pad and lst2_pad. If one list is longer than the other, the
'extra' elements (phonemes) are added at the end of the mixed list.

  #+begin_src python
train_mixed_p2vm = [
    list(filter(None, chain(*zip_longest(lst1_pad[i], lst2_pad[i]))))
    for i in range(len(lst1_pad))
]
train_mixed_p2vm[0]
  #+end_src

  #+RESULTS:
  | s | s | eh | eh | m | m | iy | ih | k | k | ow | ow | l | l | ah | ah | n | n | pad | pad |

Now I can train the embedding:

  #+begin_src python :results silent
p2vm = Word2Vec.load("models/p2vm_dict_pad.model")
  #+end_src

  #+begin_src python :results silent
EMB_DIM = 20
p2vm = Word2Vec(
    negative=30,
    sentences=train_mixed_p2vm,
    size=EMB_DIM,
    window=2,
    sg=1,
    iter=10,
    workers=multiprocessing.cpu_count(),
)
p2vm.save("models/p2vm_dict_pad.model")
  #+end_src

These are the similarity scores for phonemes I expect to be similar,
based on intuition:

  #+begin_src python
(
    p2vm.wv.similarity("ay", "oy"),
    p2vm.wv.similarity("ey", "ay"),
    p2vm.wv.similarity("uw", "aw"),
    p2vm.wv.similarity("sh", "zh"),
)
  #+end_src

  #+RESULTS:
  | 0.21737549 | 0.42717785 | 0.41636568 | 0.38868156 |

These are the similarity scores for phonemes I expect to be
dissimilar, based on intuition:

  #+begin_src python
(
    p2vm.wv.similarity("hh", "oy"),
    p2vm.wv.similarity("v", "dh"),
    p2vm.wv.similarity("z", "th"),
    p2vm.wv.similarity("w", "l"),
)
  #+end_src

  #+RESULTS:
  | 0.11504649 | 0.1797012 | 0.36657834 | 0.51769096 |

It looks like the model gives similar scores to similar sounding
phonemes and dissimilar sounding phonemes. Something that surprises me
is the very low score for "sh" and "zh", even though they are similar
sounding phonemes. The high score for "w" and "l" is also surprising,
because I do not expect them to sound similar.

*** Visualization
To create a t-SNE plot, I need all the phonemes included in the
embedding, as well as the embedding itself. The Dict data has one less
phoneme compared to the phonemized SQuAD and Subj data, namely the
'ax' phoneme:

   #+begin_src python
phonemes = list(p2vm.wv.vocab)
vocab = dict([(x,y) for (y,x) in enumerate(phonemes + ["ax"])])
vocab_dict = {"vocab": vocab, "rev": phonemes + ["ax"]}
with open("models/p2vm_dict_pad_vocab.pkl", "wb") as outfile:
    pickle.dump(vocab_dict, outfile)
idx = phonemes.index("er")
X = np.concatenate([p2vm[phonemes], p2vm[phonemes][idx].reshape((1, 20))])
np.save("models/p2vm_dict_pad.npy", X)

(phonemes, X.shape)
   #+end_src

   #+RESULTS:
   |  s | eh | m | iy | ih | k | ow | l | ah | n | pad | ey | aa | r | z | b | ae | uw | aw | d | t | ng | sh | er | y | ao | v | p | ch | uh | g | th | w | ay | jh | hh | f | oy | dh | zh |
   | 41 | 20 |   |    |    |   |    |   |    |   |     |    |    |   |   |   |    |    |    |   |   |    |    |    |   |    |   |   |    |    |   |    |   |    |    |    |   |    |    |    |

Now I can fit the t-SNE and put the results in a DataFrame:

   #+begin_src python
tsne = TSNE(n_components=2)
X_tsne = tsne.fit_transform(X)
df = pd.DataFrame(X_tsne, index=phonemes, columns=["x", "y"])

df.shape
   #+end_src

   #+RESULTS:
   | 40 | 2 |

   #+begin_src python :results silent
def annotate(row, ax):
    ax.annotate(row.name, (row.x, row.y),
                xytext=(10, -5), textcoords='offset points')
   #+end_src

Now the t-SNE can be plotted:

   #+begin_src python :results silent
ax1 = df.plot.scatter(x="x", y="y")
df.apply(annotate, ax=ax1, axis=1)
fig = ax1.get_figure()
fig.savefig("figures/p2vm_dict_pad.png")
   #+end_src

* p2va
** Needleman-Wunsch algorithm
*** The algorithm
This implementation of the Needleman-Wunsch alignment algorithm was
written by John Lekberg and found [[https://johnlekberg.com/blog/2020-10-25-seq-align.html][here]].

  #+begin_src python :results silent
def needleman_wunsch(x, y):
    """Run the Needleman-Wunsch algorithm on two sequences.

    x, y -- sequences.

    Code based on pseudocode in Section 3 of:

    Naveed, Tahir; Siddiqui, Imitaz Saeed; Ahmed, Shaftab.
    "Parallel Needleman-Wunsch Algorithm for Grid." n.d.
    https://upload.wikimedia.org/wikipedia/en/c/c4/ParallelNeedlemanAlgorithm.pdf
    """
    N, M = len(x), len(y)
    s = lambda a, b: int(a == b)
    DIAG = -1, -1
    LEFT = -1, 0
    UP = 0, -1
    # Create tables F and Ptr
    F = {}
    Ptr = {}
    F[-1, -1] = 0
    for i in range(N):
        F[i, -1] = -i

    for j in range(M):
        F[-1, j] = -j

    option_Ptr = DIAG, LEFT, UP
    for i, j in product(range(N), range(M)):
        option_F = (
            F[i - 1, j - 1] + s(x[i], y[j]),
            F[i - 1, j] - 1,
            F[i, j - 1] - 1,
        )
        F[i, j], Ptr[i, j] = max(zip(option_F, option_Ptr))

    # Work backwards from (N - 1, M - 1) to (0, 0)
    # to find the best alignment.
    alignment = deque()
    i, j = N - 1, M - 1
    while i >= 0 and j >= 0:
        direction = Ptr[i, j]
        if direction == DIAG:
            element = i, j

        elif direction == LEFT:
            element = i, None

        elif direction == UP:
            element = None, j

        alignment.appendleft(element)
        di, dj = direction
        i, j = i + di, j + dj

    while i >= 0:
        alignment.appendleft((i, None))
        i -= 1

    while j >= 0:
        alignment.appendleft((None, j))
        j -= 1

    return list(alignment)
  #+end_src

Let's try the needleman_wunsch alignment function:

  #+begin_src python
needleman_wunsch("CAT", "CT")
  #+end_src

  #+RESULTS:
  | 0 |    0 |
  | 1 | None |
  | 2 |    1 |

In terms of indices it is hard to say what this alignment looks
like. If we use the print function also given by John Lekberg:

#+begin_src python :results silent
def get_alignment(x, y, alignment):
    return (
        "".join("-" if i is None else x[i] for i, _ in alignment),
        "".join("-" if j is None else y[j] for _, j in alignment),
    )
#+end_src

#+begin_src python
get_alignment(
    ["C", "A", "T"], ["C", "T"], needleman_wunsch(["C", "A", "T"], ["C", "T"])
)
#+end_src

#+RESULTS:
| CAT | C-T |

*** Using the algorithm for phonemes
This algorithm can almost directly be applied to phonemes. The only
choice that I need to make here, is what to do with the gaps in the
alignment. I have chosen to put padding symbols in place of these gaps
to reflect the absence of sound. Aside from that, I return the
sequences as lists of strings (the phonemes/padding symbols) rather
than strings.

#+begin_src python :results silent
def get_phoneme_alignment(x, y, alignment):
    return (
        ["pad" if i is None else x[i] for i, _ in alignment],
        ["pad" if j is None else y[j] for _, j in alignment],
    )
#+end_src

Now let's try this out on two sequences of phonemes:

#+begin_src python
get_phoneme_alignment(
    train_ref[0], train_asr[0], needleman_wunsch(train_ref[0], train_asr[0])
)
#+end_src

#+RESULTS:
| hh | aw | s | pad | ah | v | pad | d | eh | r | ey | ax | n | pad | b | ih | k | ey | m | pad | n | ow | n | pad | th | r | uw | pad | b | ey | ax | n | s | pad | ae | n | d | pad | w | ih | ch | pad | ah | v | pad | b | ey | ax | n | pad | pad | s | pad | pad | ax | s | pad | r | eh | l | ax | t | ih | v | z |
| hh | aw | s | pad | ah | v | pad | d | eh | r | ey | ax | n | pad | b | ih | k | ey | m | pad | n | ow | n | pad | th | r | uw | pad | b | ey | ax | n | s | pad | ae | n | d | pad | w | ih | ch | pad | ah | v | pad | b | ih | aa | n | d   | pad | s | ey  | pad | eh | s | pad | r | eh | l | ax | t | ih | v | z |

This looks ready to use for the training of a phoneme embedding!

** SQuAD and Subj
*** Embedding
I first initialize the model:

  #+begin_src python :results silent
EMB_DIM = 20
p2va = Word2Vec(
    #negative=0,
    size=EMB_DIM,
    window=2,
    sg=1,
    iter=10,
    workers=multiprocessing.cpu_count(),
)
  #+end_src

Then set the context window:

  #+begin_src python :results silent
context_window = 0
  #+end_src

Now I align phonemized sentences and create lists of individual
phonemes and their contexts for training:

  #+begin_src python
train_aligned_p2va = []
for i in range(len(train_ref)):
    alignment = get_phoneme_alignment(
        train_ref[i], train_asr[i], needleman_wunsch(train_ref[i], train_asr[i])
    )
    ref_alignment = alignment[0]
    asr_alignment = alignment[1]
    for j in range(len(ref_alignment)):
        train_aligned_p2va.append(
            [ref_alignment[j]]
            + [
                asr_alignment[
                    max(0, j - context_window) : min(
                        j + context_window + 1, len(asr_alignment)
                    )
                ]
            ]
        )
        train_aligned_p2va.append(
            [asr_alignment[j]]
            + [
                ref_alignment[
                    max(0, j - context_window) : min(
                        j + context_window + 1, len(ref_alignment)
                    )
                ]
            ]
        )

(
    train_ref[0][48:53],
    train_asr[0][48:53],
    train_aligned_p2va[100],
    train_aligned_p2va[101],
)
  #+end_src

  #+RESULTS:

Before training I need to add the vocabulary to the Word2Vec model:

  #+begin_src python
start = len(p2va.wv.vocab)
p2va.build_vocab(train_ref + train_asr)
end = len(p2va.wv.vocab)
(start, end)
  #+end_src

  #+RESULTS:
  | 0 | 41 |

Now the model can be trained and saved:

  #+begin_src python :results silent
for sentence in tqdm.tqdm(train_aligned_p2va):
    for word in sentence[1]:
        _ = gensim.models.word2vec.train_sg_pair(
            p2va,
            sentence[0],
            p2va.wv.vocab[word].index,
            alpha=0.025,
            )

p2va.save(f"models/p2va_{context_window}_asr.model")
  #+end_src

To make the train_sg_pair function work with the fast cython based
version of gensim I had to edit one line in the word2vec.py file. I
exchanged 'model.neg_labels' for 'array([1] + [0] * model.negative)',
since the word2vec model in the fast version does not have a
neg_labels attribute.

*** Context window = 2
#+begin_src python :results silent
p2va_2 = Word2Vec.load("models/p2va_2_asr.model")
#+end_src

These are the similarity scores for phonemes I expect to be similar,
based on intuition:

  #+begin_src python
(
    p2va_2.wv.similarity("ay", "oy"),
    p2va_2.wv.similarity("ey", "ay"),
    p2va_2.wv.similarity("uw", "aw"),
    p2va_2.wv.similarity("sh", "zh"),
)
  #+end_src

  #+RESULTS:
  | 0.62876457 | 0.79092807 | 0.7093742 | 0.68627286 |


These are the similarity scores for phonemes I expect to be
dissimilar, based on intuition:

  #+begin_src python
(
    p2va_2.wv.similarity("hh", "oy"),
    p2va_2.wv.similarity("v", "dh"),
    p2va_2.wv.similarity("z", "th"),
    p2va_2.wv.similarity("w", "l"),
)
  #+end_src

  #+RESULTS:
  | 0.55439144 | 0.83803666 | 0.79349804 | 0.80255985 |

**** Visualization
To create a t-SNE plot, I need all the phonemes included in the
embedding, as well as the embedding itself:

   #+begin_src python
phonemes = list(p2va_2.wv.vocab)
vocab = dict([(x,y) for (y,x) in enumerate(phonemes)])
vocab_dict = {"vocab": vocab, "rev": phonemes}
with open("models/p2va_2_asr_vocab.pkl", "wb") as outfile:
    pickle.dump(vocab_dict, outfile)
X = p2va_2[phonemes]
np.save("models/p2va_2_asr.npy", X)

(phonemes, X.shape)
   #+end_src

   #+RESULTS:
   | hh | aw | s | pad | ah | v | d | eh | r | ey | ax | n | b | ih | k | m | ow | th | uw | ae | w | ch | l | t | z | aa | f | ao | er | p | sh | ng | ay | uh | y | iy | g | dh | jh | oy | zh |
   | 41 | 20 |   |     |    |   |   |    |   |    |    |   |   |    |   |   |    |    |    |    |   |    |   |   |   |    |   |    |    |   |    |    |    |    |   |    |   |    |    |    |    |

Now I can fit the t-SNE and put the results in a DataFrame:

   #+begin_src python
tsne = TSNE(n_components=2)
X_tsne = tsne.fit_transform(X)
df = pd.DataFrame(X_tsne, index=phonemes, columns=["x", "y"])

df.shape
   #+end_src

   #+RESULTS:
   | 41 | 2 |

   #+begin_src python :results silent
def annotate(row, ax):
    ax.annotate(row.name, (row.x, row.y),
                xytext=(10, -5), textcoords='offset points')
   #+end_src

Now the t-SNE can be plotted:

   #+begin_src python :results silent
ax1 = df.plot.scatter(x="x", y="y")
df.apply(annotate, ax=ax1, axis=1)
fig = ax1.get_figure()
fig.savefig("p2va_2.png")
#+end_src

*** Context window = 0
#+begin_src python :results silent
p2va_0 = Word2Vec.load("models/p2va_0_asr.model")
#+end_src

These are the similarity scores for phonemes I expect to be similar,
based on intuition:

  #+begin_src python
(
    p2va_0.wv.similarity("ay", "oy"),
    p2va_0.wv.similarity("ey", "ay"),
    p2va_0.wv.similarity("uw", "aw"),
    p2va_0.wv.similarity("sh", "zh"),
)
  #+end_src

  #+RESULTS:
  | 0.12977214 | 0.39927554 | 0.13998131 | 0.036319654 |



These are the similarity scores for phonemes I expect to be
dissimilar, based on intuition:

  #+begin_src python
(
    p2va_0.wv.similarity("hh", "oy"),
    p2va_0.wv.similarity("v", "dh"),
    p2va_0.wv.similarity("z", "th"),
    p2va_0.wv.similarity("w", "l"),
)
  #+end_src

  #+RESULTS:
  | 0.14004605 | -0.22204834 | -0.25583318 | 0.30523828 |

**** Visualization
To create a t-SNE plot, I need all the phonemes included in the
embedding, as well as the embedding itself:

   #+begin_src python
phonemes = list(p2va_0.wv.vocab)
vocab = dict([(x,y) for (y,x) in enumerate(phonemes)])
vocab_dict = {"vocab": vocab, "rev": phonemes}
with open("models/p2va_0_asr_vocab.pkl", "wb") as outfile:
    pickle.dump(vocab_dict, outfile)
X = p2va_0[phonemes]
np.save("models/p2va_0_asr.npy", X)

(phonemes, X.shape)
   #+end_src

   #+RESULTS:
   | hh | aw | s | pad | ah | v | d | eh | r | ey | ax | n | b | ih | k | m | ow | th | uw | ae | w | ch | l | t | z | aa | f | ao | er | p | sh | ng | ay | uh | y | iy | g | dh | jh | oy | zh |
   | 41 | 20 |   |     |    |   |   |    |   |    |    |   |   |    |   |   |    |    |    |    |   |    |   |   |   |    |   |    |    |   |    |    |    |    |   |    |   |    |    |    |    |

Now I can fit the t-SNE and put the results in a DataFrame:

   #+begin_src python
tsne = TSNE(n_components=2)
X_tsne = tsne.fit_transform(X)
df = pd.DataFrame(X_tsne, index=phonemes, columns=["x", "y"])

df.shape
   #+end_src

   #+RESULTS:
   | 41 | 2 |

   #+begin_src python :results silent
def annotate(row, ax):
    ax.annotate(row.name, (row.x, row.y),
                xytext=(10, -5), textcoords='offset points')
   #+end_src

Now the t-SNE can be plotted:

   #+begin_src python :results silent
ax1 = df.plot.scatter(x="x", y="y")
df.apply(annotate, ax=ax1, axis=1)
fig = ax1.get_figure()
fig.savefig("p2va_0.png")
   #+end_src

** Dict
*** Embedding
I first initialize the model:

  #+begin_src python :results silent
EMB_DIM = 20
p2va = Word2Vec(
    #negative=0,
    size=EMB_DIM,
    window=2,
    sg=1,
    iter=10,
    workers=multiprocessing.cpu_count(),
)
  #+end_src

Then set the context window:

  #+begin_src python :results silent
context_window = 0
  #+end_src

Now I align phonemized sentences and create lists of individual
phonemes and their contexts for training:

  #+begin_src python
train_aligned_p2va = []
for i in range(len(lst1)):
    alignment = get_phoneme_alignment(
        lst1[i], lst2[i], needleman_wunsch(lst1[i], lst2[i])
    )
    lst1_alignment = alignment[0]
    lst2_alignment = alignment[1]
    for j in range(len(lst1_alignment)):
        train_aligned_p2va.append(
            [lst1_alignment[j]]
            + [
                lst2_alignment[
                    max(0, j - context_window) : min(
                        j + context_window + 1, len(lst2_alignment)
                    )
                ]
            ]
        )
        train_aligned_p2va.append(
            [lst2_alignment[j]]
            + [
                lst1_alignment[
                    max(0, j - context_window) : min(
                        j + context_window + 1, len(lst1_alignment)
                    )
                ]
            ]
        )

(
    lst1[0][0:5],
    lst2[0][0:5],
    train_aligned_p2va[6],
    train_aligned_p2va[7],
)
  #+end_src

  #+RESULTS:
  | s  | eh   | m | iy | k |
  | s  | eh   | m | ih | k |
  | iy | (ih) |   |    |   |
  | ih | (iy) |   |    |   |

Before training I need to add the vocabulary to the Word2Vec model:

  #+begin_src python
start = len(p2va.wv.vocab)
p2va.build_vocab([["pad", "pad", "pad", "pad", "pad"]] + lst1 + lst2)
end = len(p2va.wv.vocab)
(start, end)
  #+end_src

  #+RESULTS:
  | 0 | 40 |

Now the model can be trained and saved:

  #+begin_src python :results silent
for sentence in tqdm.tqdm(train_aligned_p2va):
    for word in sentence[1]:
        _ = gensim.models.word2vec.train_sg_pair(
            p2va,
            sentence[0],
            p2va.wv.vocab[word].index,
            alpha=0.025,
            )

p2va.save(f"models/p2va_{context_window}_dict.model")
  #+end_src

To make the train_sg_pair function work with the fast cython based
version of gensim I had to edit one line in the word2vec.py file. I
exchanged 'model.neg_labels' for 'array([1] + [0] * model.negative)',
since the word2vec model in the fast version does not have a
neg_labels attribute.

*** Context window = 2
#+begin_src python :results silent
p2va_2 = Word2Vec.load("models/p2va_2_dict.model")
#+end_src

These are the similarity scores for phonemes I expect to be similar,
based on intuition:

  #+begin_src python
(
    p2va_2.wv.similarity("ay", "oy"),
    p2va_2.wv.similarity("ey", "ay"),
    p2va_2.wv.similarity("uw", "aw"),
    p2va_2.wv.similarity("sh", "zh"),
)
  #+end_src

  #+RESULTS:
  | 0.7323048 | 0.88144827 | 0.8737555 | 0.79604393 |

These are the similarity scores for phonemes I expect to be
dissimilar, based on intuition:

  #+begin_src python
(
    p2va_2.wv.similarity("hh", "oy"),
    p2va_2.wv.similarity("v", "dh"),
    p2va_2.wv.similarity("z", "th"),
    p2va_2.wv.similarity("w", "l"),
)
  #+end_src

  #+RESULTS:
  | 0.79544413 | 0.7986341 | 0.84386724 | 0.82617694 |

**** Visualization
To create a t-SNE plot, I need all the phonemes included in the
embedding, as well as the embedding itself. The Dict data has one less
phoneme compared to the phonemized SQuAD and Subj data, namely the
'ax' phoneme:

   #+begin_src python
phonemes = list(p2va_2.wv.vocab)
vocab = dict([(x,y) for (y,x) in enumerate(phonemes + ["ax"])])
vocab_dict = {"vocab": vocab, "rev": phonemes + ["ax"]}
with open("models/p2va_2_dict_vocab.pkl", "wb") as outfile:
    pickle.dump(vocab_dict, outfile)
idx = phonemes.index("er")
X = np.concatenate([p2va_2[phonemes], p2va_2[phonemes][idx].reshape((1, 20))])

np.save("models/p2va_2_dict.npy", X)

(phonemes, X.shape)
   #+end_src

   #+RESULTS:
   | pad |  s | eh | m | iy | k | ow | l | ah | n | r | z | b | aa | ae | uw | d | t | ih | ng | sh | er | y | ey | ao | v | p | ch | g | aw | w | ay | jh | hh | f | th | uh | oy | dh | zh |
   |  41 | 20 |    |   |    |   |    |   |    |   |   |   |   |    |    |    |   |   |    |    |    |    |   |    |    |   |   |    |   |    |   |    |    |    |   |    |    |    |    |    |

Now I can fit the t-SNE and put the results in a DataFrame:

   #+begin_src python
tsne = TSNE(n_components=2)
X_tsne = tsne.fit_transform(X)
df = pd.DataFrame(X_tsne, index=phonemes, columns=["x", "y"])

df.shape
   #+end_src

   #+RESULTS:
   | 40 | 2 |

   #+begin_src python :results silent
def annotate(row, ax):
    ax.annotate(row.name, (row.x, row.y),
                xytext=(10, -5), textcoords='offset points')
   #+end_src

Now the t-SNE can be plotted:

   #+begin_src python :results silent
ax1 = df.plot.scatter(x="x", y="y")
df.apply(annotate, ax=ax1, axis=1)
fig = ax1.get_figure()
fig.savefig("p2va_2_dict.png")
#+end_src

*** Context window = 0
#+begin_src python :results silent
p2va_0 = Word2Vec.load("models/p2va_0_dict.model")
#+end_src

These are the similarity scores for phonemes I expect to be similar,
based on intuition:

  #+begin_src python
(
    p2va_0.wv.similarity("ay", "oy"),
    p2va_0.wv.similarity("ey", "ay"),
    p2va_0.wv.similarity("uw", "aw"),
    p2va_0.wv.similarity("sh", "zh"),
)
  #+end_src

  #+RESULTS:
  | 0.4840838 | 0.7190375 | 0.76230735 | 0.7766007 |

These are the similarity scores for phonemes I expect to be
dissimilar, based on intuition:

  #+begin_src python
(
    p2va_0.wv.similarity("hh", "oy"),
    p2va_0.wv.similarity("v", "dh"),
    p2va_0.wv.similarity("z", "th"),
    p2va_0.wv.similarity("w", "l"),
)
  #+end_src

  #+RESULTS:
  | 0.735503 | 0.7481821 | 0.6242971 | 0.505862 |

**** Visualization
To create a t-SNE plot, I need all the phonemes included in the
embedding, as well as the embedding itself. The Dict data has one less
phoneme compared to the phonemized SQuAD and Subj data, namely the
'ax' phoneme:

   #+begin_src python
phonemes = list(p2va_0.wv.vocab)
vocab = dict([(x,y) for (y,x) in enumerate(phonemes + ["ax"])])
vocab_dict = {"vocab": vocab, "rev": phonemes + ["ax"]}
with open("models/p2va_0_dict_vocab.pkl", "wb") as outfile:
    pickle.dump(vocab_dict, outfile)
idx = phonemes.index("ao")
X = np.concatenate([p2va_0[phonemes], p2va_0[phonemes][idx].reshape((1, 20))])
np.save("models/p2va_0_dict.npy", X)

(phonemes, X.shape)
   #+end_src

   #+RESULTS:
   | pad |  s | eh | m | iy | k | ow | l | ah | n | r | z | b | aa | ae | uw | d | t | ih | ng | sh | er | y | ey | ao | v | p | ch | g | aw | w | ay | jh | hh | f | th | uh | oy | dh | zh |
   |  41 | 20 |    |   |    |   |    |   |    |   |   |   |   |    |    |    |   |   |    |    |    |    |   |    |    |   |   |    |   |    |   |    |    |    |   |    |    |    |    |    |

Now I can fit the t-SNE and put the results in a DataFrame:

   #+begin_src python
tsne = TSNE(n_components=2)
X_tsne = tsne.fit_transform(X)
df = pd.DataFrame(X_tsne, index=phonemes, columns=["x", "y"])

df.shape
   #+end_src

   #+RESULTS:
   | 40 | 2 |

   #+begin_src python :results silent
def annotate(row, ax):
    ax.annotate(row.name, (row.x, row.y),
                xytext=(10, -5), textcoords='offset points')
   #+end_src

Now the t-SNE can be plotted:

   #+begin_src python :results silent
ax1 = df.plot.scatter(x="x", y="y")
df.apply(annotate, ax=ax1, axis=1)
fig = ax1.get_figure()
fig.savefig("p2va_0_dict.png")
   #+end_src

** Dict_pad
*** Embedding
I first initialize the model:

  #+begin_src python :results silent
EMB_DIM = 20
p2va = Word2Vec(
    #negative=0,
    size=EMB_DIM,
    window=2,
    sg=1,
    iter=10,
    workers=multiprocessing.cpu_count(),
)
  #+end_src

Then set the context window:

  #+begin_src python :results silent
context_window = 2
  #+end_src

Now I align phonemized sentences and create lists of individual
phonemes and their contexts for training:

  #+begin_src python
train_aligned_p2va = []
for i in range(len(lst1)):
    alignment = get_phoneme_alignment(
        lst1_pad[i], lst2_pad[i], needleman_wunsch(lst1_pad[i], lst2_pad[i])
    )
    lst1_pad_alignment = alignment[0]
    lst2_pad_alignment = alignment[1]
    for j in range(len(lst1_pad_alignment)):
        train_aligned_p2va.append(
            [lst1_pad_alignment[j]]
                   + [
                lst2_pad_alignment[
                    max(0, j - context_window) : min(
                        j + context_window + 1, len(lst2_pad_alignment)
                    )
                ]
            ]
        )
        train_aligned_p2va.append(
            [lst2_pad_alignment[j]]
            + [
                lst1_pad_alignment[
                    max(0, j - context_window) : min(
                        j + context_window + 1, len(lst1_pad_alignment)
                    )
                ]
            ]
        )

(
    lst1_pad[0][0:10],
    lst2_pad[0][0:10],
    train_aligned_p2va[6],
    train_aligned_p2va[7],
)
  #+end_src

  #+RESULTS:
  | s  | eh             | m | iy | k | ow | l | ah | n | pad |
  | s  | eh             | m | ih | k | ow | l | ah | n | pad |
  | iy | (eh m ih k ow) |   |    |   |    |   |    |   |     |
  | ih | (eh m iy k ow) |   |    |   |    |   |    |   |     |

Before training I need to add the vocabulary to the Word2Vec model:

  #+begin_src python
start = len(p2va.wv.vocab)
p2va.build_vocab(lst1_pad + lst2_pad)
end = len(p2va.wv.vocab)
(start, end)
  #+end_src

  #+RESULTS:
  | 0 | 40 |

Now the model can be trained and saved:

  #+begin_src python :results silent
for sentence in tqdm.tqdm(train_aligned_p2va):
    for word in sentence[1]:
        _ = gensim.models.word2vec.train_sg_pair(
            p2va,
            sentence[0],
            p2va.wv.vocab[word].index,
            alpha=0.025,
            )

p2va.save(f"models/p2va_{context_window}_dict_pad.model")
  #+end_src

To make the train_sg_pair function work with the fast cython based
version of gensim I had to edit one line in the word2vec.py file. I
exchanged 'model.neg_labels' for 'array([1] + [0] * model.negative)',
since the word2vec model in the fast version does not have a
neg_labels attribute.

*** Context window = 2
#+begin_src python :results silent
p2va_2 = Word2Vec.load("models/p2va_2_dict_pad.model")
#+end_src

These are the similarity scores for phonemes I expect to be similar,
based on intuition:

  #+begin_src python
(
    p2va_2.wv.similarity("ay", "oy"),
    p2va_2.wv.similarity("ey", "ay"),
    p2va_2.wv.similarity("uw", "aw"),
    p2va_2.wv.similarity("sh", "zh"),
)
  #+end_src

  #+RESULTS:
  | 0.8294416 | 0.8794624 | 0.8637798 | 0.8030078 |

These are the similarity scores for phonemes I expect to be
dissimilar, based on intuition:

  #+begin_src python
(
    p2va_2.wv.similarity("hh", "oy"),
    p2va_2.wv.similarity("v", "dh"),
    p2va_2.wv.similarity("z", "th"),
    p2va_2.wv.similarity("w", "l"),
)
  #+end_src

  #+RESULTS:
  | 0.7381566 | 0.8203041 | 0.83652234 | 0.85238844 |

**** Visualization
To create a t-SNE plot, I need all the phonemes included in the
embedding, as well as the embedding itself. The Dict data has one less
phoneme compared to the phonemized SQuAD and Subj data, namely the
'ax' phoneme:

   #+begin_src python
phonemes = list(p2va_2.wv.vocab)
vocab = dict([(x,y) for (y,x) in enumerate(phonemes + ["ax"])])
vocab_dict = {"vocab": vocab, "rev": phonemes + ["ax"]}
with open("models/p2va_2_dict_pad_vocab.pkl", "wb") as outfile:
    pickle.dump(vocab_dict, outfile)
idx = phonemes.index("er")
X = np.concatenate([p2va_2[phonemes], p2va_2[phonemes][idx].reshape((1, 20))])
np.save("models/p2va_2_dict_pad.npy", X)

(phonemes, X.shape)
   #+end_src

   #+RESULTS:
   |  s | eh | m | iy | k | ow | l | ah | n | pad | r | z | b | aa | ae | uw | d | t | ih | ng | sh | er | y | ey | ao | v | p | ch | g | aw | w | ay | jh | hh | f | th | uh | oy | dh | zh |
   | 41 | 20 |   |    |   |    |   |    |   |     |   |   |   |    |    |    |   |   |    |    |    |    |   |    |    |   |   |    |   |    |   |    |    |    |   |    |    |    |    |    |

Now I can fit the t-SNE and put the results in a DataFrame:

   #+begin_src python
tsne = TSNE(n_components=2)
X_tsne = tsne.fit_transform(X)
df = pd.DataFrame(X_tsne, index=phonemes, columns=["x", "y"])

df.shape
   #+end_src

   #+RESULTS:
   | 40 | 2 |

   #+begin_src python :results silent
def annotate(row, ax):
    ax.annotate(row.name, (row.x, row.y),
                xytext=(10, -5), textcoords='offset points')
   #+end_src

Now the t-SNE can be plotted:

   #+begin_src python :results silent
ax1 = df.plot.scatter(x="x", y="y")
df.apply(annotate, ax=ax1, axis=1)
fig = ax1.get_figure()
fig.savefig("p2va_2_dict_pad.png")
#+end_src

*** Context window = 0
#+begin_src python :results silent
p2va_0 = Word2Vec.load("models/p2va_0_dict_pad.model")
#+end_src

These are the similarity scores for phonemes I expect to be similar,
based on intuition:

  #+begin_src python
(
    p2va_0.wv.similarity("ay", "oy"),
    p2va_0.wv.similarity("ey", "ay"),
    p2va_0.wv.similarity("uw", "aw"),
    p2va_0.wv.similarity("sh", "zh"),
)
  #+end_src

  #+RESULTS:
  | 0.70183736 | 0.79159486 | 0.7826972 | 0.7395877 |

These are the similarity scores for phonemes I expect to be
dissimilar, based on intuition:

  #+begin_src python
(
    p2va_0.wv.similarity("hh", "oy"),
    p2va_0.wv.similarity("v", "dh"),
    p2va_0.wv.similarity("z", "th"),
    p2va_0.wv.similarity("w", "l"),
)
  #+end_src

  #+RESULTS:
  | 0.6647972 | 0.7729586 | 0.6467948 | 0.6428419 |

**** Visualization
To create a t-SNE plot, I need all the phonemes included in the
embedding, as well as the embedding itself. The Dict data has one less
phoneme compared to the phonemized SQuAD and Subj data, namely the
'ax' phoneme:

   #+begin_src python
phonemes = list(p2va_0.wv.vocab)
vocab = dict([(x,y) for (y,x) in enumerate(phonemes + ["ax"])])
vocab_dict = {"vocab": vocab, "rev": phonemes + ["ax"]}
with open("models/p2va_0_dict_pad_vocab.pkl", "wb") as outfile:
    pickle.dump(vocab_dict, outfile)
idx = phonemes.index("ao")
X = np.concatenate([p2va_0[phonemes], p2va_0[phonemes][idx].reshape((1, 20))])
np.save("models/p2va_0_dict_pad.npy", X)

(phonemes, X.shape)
   #+end_src

   #+RESULTS:
   |  s | eh | m | iy | k | ow | l | ah | n | pad | r | z | b | aa | ae | uw | d | t | ih | ng | sh | er | y | ey | ao | v | p | ch | g | aw | w | ay | jh | hh | f | th | uh | oy | dh | zh |
   | 41 | 20 |   |    |   |    |   |    |   |     |   |   |   |    |    |    |   |   |    |    |    |    |   |    |    |   |   |    |   |    |   |    |    |    |   |    |    |    |    |    |

Now I can fit the t-SNE and put the results in a DataFrame:

   #+begin_src python
tsne = TSNE(n_components=2)
X_tsne = tsne.fit_transform(X)
df = pd.DataFrame(X_tsne, index=phonemes, columns=["x", "y"])

df.shape
   #+end_src

   #+RESULTS:
   | 40 | 2 |

   #+begin_src python :results silent
def annotate(row, ax):
    ax.annotate(row.name, (row.x, row.y),
                xytext=(10, -5), textcoords='offset points')
   #+end_src

Now the t-SNE can be plotted:

   #+begin_src python :results silent
ax1 = df.plot.scatter(x="x", y="y")
df.apply(annotate, ax=ax1, axis=1)
fig = ax1.get_figure()
fig.savefig("p2va_0_dict_pad.png")
   #+end_src
* 3D t-sne
To create a 3D t-SNE plot, I need all the phonemes included in the
embedding, as well as the embedding itself:

  #+begin_src python
for embedding in ["p2vc_asr", "p2vm_asr"]:
    p2vc = Word2Vec.load(f"models/{embedding}.model")
    X = p2vc[phonemes]
    tsne = TSNE(n_components=3)
    X_tsne = tsne.fit_transform(X)
    df = pd.DataFrame(X_tsne, index=phonemes, columns=["x", "y", "z"])
    fig = figure(figsize=(12, 9))
    ax = Axes3D(fig)
    counter = 0
    for i, row in df.iterrows():
        ax.scatter(row["x"], row["y"], row["z"], color="b")
        if counter % 2 == 0:
            ax.text(
                row["x"] - 15,
                row["y"] - 15,
                row["z"] - 30,
                "%s" % (str(i)),
                size=10,
                zorder=1,
                color="k",
            )

        else:
            ax.text(
                row["x"] + 15,
                row["y"] + 15,
                row["z"] + 10,
                "%s" % (str(i)),
                size=10,
                zorder=1,
                color="k",
            )

    counter += 1
    ax.set_xlabel("x")
    ax.set_ylabel("y")
    ax.set_zlabel("z")
    fig.savefig(f"figures/{embedding}_3d.png")
  #+end_src
